{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4bd2b1d",
   "metadata": {},
   "source": [
    "### 1.\n",
    "Importing data and splitting into test, validation and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built arrays: X shape = (9404, 14), Y shape = (9404, 3), unique groups = 362\n",
      "Split sizes (samples): train=6595, val=1415, test=1394\n",
      "Torch tensors: torch.Size([6595, 14]) torch.Size([6595, 3]) torch.Size([1415, 14]) torch.Size([1415, 3]) torch.Size([1394, 14]) torch.Size([1394, 3])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "TRAIN_FRAC, VAL_FRAC, TEST_FRAC = 0.70, 0.15, 0.15\n",
    "SEED = 42\n",
    "DTYPE = np.float32\n",
    "\n",
    "#-------------------------------------------------------- Utils --------------------------------------------------#\n",
    "def fit_minmax(X):\n",
    "    \"\"\"Fit per-feature min-max on X (2D). Returns (mins, ranges) with safe ranges.\"\"\"\n",
    "    mins = X.min(axis=0)\n",
    "    maxs = X.max(axis=0)\n",
    "    rng = maxs - mins\n",
    "    rng_safe = np.where(rng > 0, rng, 1.0)  # avoid division by zero for constant features\n",
    "    return mins, rng_safe\n",
    "\n",
    "def transform_minmax(X, mins, rng_safe):\n",
    "    return (X - mins) / rng_safe\n",
    "\n",
    "def inverse_minmax(X_scaled, mins, rng_safe):\n",
    "    return X_scaled * rng_safe + mins\n",
    "\n",
    "def split_by_groups(groups, train_frac, val_frac, test_frac, seed=42):\n",
    "    \"\"\"\n",
    "    Split indices by unique group IDs (e.g., df row index). All samples from a group\n",
    "    go to the same split. Returns boolean masks for train/val/test of length len(groups).\n",
    "    \"\"\"\n",
    "    assert abs((train_frac + val_frac + test_frac) - 1.0) < 1e-8, \"Fractions must sum to 1.\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    unique_groups = np.unique(groups)\n",
    "    rng.shuffle(unique_groups)\n",
    "\n",
    "    n = len(unique_groups)\n",
    "    n_train = int(round(train_frac * n))\n",
    "    n_val = int(round(val_frac * n))\n",
    "    # assign the remainder to test to guarantee coverage of all groups\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train_groups = set(unique_groups[:n_train])\n",
    "    val_groups   = set(unique_groups[n_train:n_train + n_val])\n",
    "    test_groups  = set(unique_groups[n_train + n_val:])\n",
    "\n",
    "    train_mask = np.array([g in train_groups for g in groups], dtype=bool)\n",
    "    val_mask   = np.array([g in val_groups for g in groups], dtype=bool)\n",
    "    test_mask  = np.array([g in test_groups for g in groups], dtype=bool)\n",
    "\n",
    "    return train_mask, val_mask, test_mask\n",
    "#-----------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "filename = 'sz.csv'\n",
    "df = pd.read_csv(os.path.join(\"Input\", filename))\n",
    "input_list = []\n",
    "output_list = []\n",
    "group_ids = []  # keep the df row index for group-aware splitting\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # Inputs\n",
    "    dV_ges = float(row[\"dV_ges\"]) / 3.6 * 1e-6\n",
    "    eps_0 = float(row[\"eps_0\"])\n",
    "    phi_0 = float(row[\"phi_0\"])\n",
    "    h_dis_0 = float(row[\"h_dis_0\"])\n",
    "    h_c_0 = float(row[\"h_c_0\"])\n",
    "    rho_c = float(row[\"rho_c\"])\n",
    "    rho_d = float(row[\"rho_d\"])\n",
    "    eta_c = float(row[\"eta_c\"])\n",
    "    eta_d = float(row[\"etc_d\"])\n",
    "    sigma = float(row[\"sigma\"])\n",
    "    T = float(row[\"T\"])\n",
    "    r_s_star = float(row[\"r_S_star\"])\n",
    "    h_p_star = float(row[\"h_p_star\"])\n",
    "    DPZ_pos = np.fromstring(row['DPZ_pos_ivgu'].strip(\"[]\"), sep=\",\") / 100\n",
    "    # Outputs\n",
    "    DPZ_height = np.fromstring(row['DPZ_height_ivgu'].strip(\"[]\"), sep=\",\") / 1000\n",
    "    DPZ_bot = np.fromstring(row['DPZ_bot_ivgu'].strip(\"[]\"), sep=\",\") / 1000\n",
    "    # Build a mask where all arrays are not NaN at the same index\n",
    "    mask = ~(np.isnan(DPZ_pos) | np.isnan(DPZ_height) | np.isnan(DPZ_bot))\n",
    "    DPZ_pos = DPZ_pos[mask]\n",
    "    DPZ_height = DPZ_height[mask]\n",
    "    DPZ_bot = DPZ_bot[mask]\n",
    "    lam = float(row[\"lambda\"])\n",
    "\n",
    "    for i in range(len(DPZ_pos)):\n",
    "        inp = [dV_ges, eps_0, phi_0, h_dis_0, h_c_0, rho_c, rho_d, \n",
    "               eta_c, eta_d, sigma, T, r_s_star, h_p_star, DPZ_pos[i]]\n",
    "        out = [lam, DPZ_height[i], DPZ_bot[i]]\n",
    "        input_list.append(inp)\n",
    "        output_list.append(out)\n",
    "        group_ids.append(idx)  # same df row id for all generated points\n",
    "\n",
    "X = np.array(input_list, dtype=np.float32)\n",
    "Y = np.array(output_list, dtype=np.float32)\n",
    "groups = np.array(group_ids)\n",
    "\n",
    "print(f\"Built arrays: X shape = {X.shape}, Y shape = {Y.shape}, unique groups = {len(np.unique(groups))}\")\n",
    "\n",
    "\n",
    "#-------------------------------------------------------- Splitting --------------------------------------------------#\n",
    "train_mask, val_mask, test_mask = split_by_groups(groups, TRAIN_FRAC, VAL_FRAC, TEST_FRAC, seed=SEED)\n",
    "\n",
    "X_train, Y_train = X[train_mask], Y[train_mask]\n",
    "X_val,   Y_val   = X[val_mask],   Y[val_mask]\n",
    "X_test,  Y_test  = X[test_mask],  Y[test_mask]\n",
    "\n",
    "print(\"Split sizes (samples):\",f\"train={len(X_train)}, val={len(X_val)}, test={len(X_test)}\")\n",
    "#---------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#---------------------------------- Normalization only on training dataset -------------------------------------------#\n",
    "x_mins, x_rng = fit_minmax(X_train)\n",
    "y_mins, y_rng = fit_minmax(Y_train)\n",
    "\n",
    "X_train_n = transform_minmax(X_train, x_mins, x_rng)\n",
    "X_val_n   = transform_minmax(X_val,   x_mins, x_rng)\n",
    "X_test_n  = transform_minmax(X_test,  x_mins, x_rng)\n",
    "\n",
    "Y_train_n = transform_minmax(Y_train, y_mins, y_rng)\n",
    "Y_val_n   = transform_minmax(Y_val,   y_mins, y_rng)\n",
    "Y_test_n  = transform_minmax(Y_test,  y_mins, y_rng)\n",
    "# ---------------------------\n",
    "# Convert to torch tensors\n",
    "# ---------------------------\n",
    "X_train_t = torch.from_numpy(X_train_n)\n",
    "Y_train_t = torch.from_numpy(Y_train_n)\n",
    "X_val_t   = torch.from_numpy(X_val_n)\n",
    "Y_val_t   = torch.from_numpy(Y_val_n)\n",
    "X_test_t  = torch.from_numpy(X_test_n)\n",
    "Y_test_t  = torch.from_numpy(Y_test_n)\n",
    "\n",
    "print(\"Torch tensors:\", X_train_t.shape, Y_train_t.shape, X_val_t.shape, Y_val_t.shape, X_test_t.shape, Y_test_t.shape)\n",
    "#---------------------------------------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367a80d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def fit_minmax(X):\n",
    "    \"\"\"Fit per-feature min-max on X (2D). Returns (mins, ranges) with safe ranges.\"\"\"\n",
    "    mins = X.min(axis=0)\n",
    "    maxs = X.max(axis=0)\n",
    "    rng = maxs - mins\n",
    "    rng_safe = np.where(rng > 0, rng, 1.0)  # avoid division by zero (constant features)\n",
    "    return mins.astype(DTYPE), rng_safe.astype(DTYPE)\n",
    "\n",
    "def transform_minmax(X, mins, rng_safe):\n",
    "    return ((X - mins) / rng_safe).astype(DTYPE)\n",
    "\n",
    "def inverse_minmax(X_scaled, mins, rng_safe):\n",
    "    return (X_scaled * rng_safe + mins).astype(DTYPE)\n",
    "\n",
    "def split_dataframe_rows(df, train_frac, val_frac, test_frac, seed=42):\n",
    "    \"\"\"Split the *rows* of the original df into train/val/test by fraction.\"\"\"\n",
    "    assert abs((train_frac + val_frac + test_frac) - 1.0) < 1e-8, \"Fractions must sum to 1.\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = df.index.to_numpy()\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    n = len(idx)\n",
    "    n_train = int(round(train_frac * n))\n",
    "    n_val = int(round(val_frac * n))\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train_idx = idx[:n_train]\n",
    "    val_idx   = idx[n_train:n_train + n_val]\n",
    "    test_idx  = idx[n_train + n_val:]\n",
    "\n",
    "    return df.loc[train_idx], df.loc[val_idx], df.loc[test_idx]\n",
    "\n",
    "\n",
    "def expand_split(df_split):\n",
    "    \"\"\"\n",
    "    Expand a split of the original df into (X, Y) arrays by iterating each row,\n",
    "    parsing vector fields, masking NaNs per-row, and stacking all samples.\n",
    "    \"\"\"\n",
    "    inputs, outputs = [], []\n",
    "\n",
    "    for _, row in df_split.iterrows():\n",
    "        # ---- scalar inputs (with the same preprocessing as in your pipeline) ----\n",
    "        dV_ges   = float(row[\"dV_ges\"]) / 3.6 * 1e-6\n",
    "        eps_0    = float(row[\"eps_0\"])\n",
    "        phi_0    = float(row[\"phi_0\"])\n",
    "        h_dis_0  = float(row[\"h_dis_0\"])\n",
    "        h_c_0    = float(row[\"h_c_0\"])\n",
    "        rho_c    = float(row[\"rho_c\"])\n",
    "        rho_d    = float(row[\"rho_d\"])\n",
    "        eta_c    = float(row[\"eta_c\"])\n",
    "        eta_d    = float(row[\"etc_d\"])\n",
    "        sigma    = float(row[\"sigma\"])\n",
    "        T        = float(row[\"T\"])\n",
    "        r_s_star = float(row[\"r_S_star\"])\n",
    "        h_p_star = float(row[\"h_p_star\"])\n",
    "        lam      = float(row[\"lambda\"])\n",
    "\n",
    "        # ---- vector fields (convert units like before) ----\n",
    "        DPZ_pos    = np.fromstring(str(row['DPZ_pos_ivgu']).strip(\"[]\"), sep=\",\") / 100.0     # m\n",
    "        DPZ_height = np.fromstring(str(row['DPZ_height_ivgu']).strip(\"[]\"), sep=\",\") / 1000.0 # m\n",
    "        DPZ_bot    = np.fromstring(str(row['DPZ_bot_ivgu']).strip(\"[]\"), sep=\",\") / 1000.0   # m\n",
    "\n",
    "        # ---- per-row NaN mask to keep only valid aligned entries ----\n",
    "        mask = ~(np.isnan(DPZ_pos) | np.isnan(DPZ_height) | np.isnan(DPZ_bot))\n",
    "        if not np.any(mask):\n",
    "            continue  # skip row if nothing valid\n",
    "\n",
    "        DPZ_pos_v    = DPZ_pos[mask]\n",
    "        DPZ_height_v = DPZ_height[mask]\n",
    "        DPZ_bot_v    = DPZ_bot[mask]\n",
    "\n",
    "        # ---- expand: one sample per DPZ position ----\n",
    "        const_feats = [dV_ges, eps_0, phi_0, h_dis_0, h_c_0, rho_c, rho_d,\n",
    "                       eta_c, eta_d, sigma, T, r_s_star, h_p_star]\n",
    "        for i in range(len(DPZ_pos_v)):\n",
    "            x_vec = const_feats + [DPZ_pos_v[i]]\n",
    "            y_vec = [lam, DPZ_height_v[i], DPZ_bot_v[i]]\n",
    "            inputs.append(x_vec)\n",
    "            outputs.append(y_vec)\n",
    "\n",
    "    X = np.array(inputs, dtype=DTYPE)\n",
    "    Y = np.array(outputs, dtype=DTYPE)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53274779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row splits: train=253, val=54, test=55\n",
      "Expanded sample counts: train=6595, val=1415, test=1394\n",
      "Feature/Target dims: 14 3\n",
      "Torch tensors: torch.Size([6595, 14]) torch.Size([6595, 3]) torch.Size([1415, 14]) torch.Size([1415, 3]) torch.Size([1394, 14]) torch.Size([1394, 3])\n"
     ]
    }
   ],
   "source": [
    "TRAIN_FRAC, VAL_FRAC, TEST_FRAC = 0.70, 0.15, 0.15\n",
    "SEED = 42\n",
    "DTYPE = np.float32\n",
    "CSV_PATH = os.path.join(\"Input\", \"sz.csv\")\n",
    "\n",
    "# 1) Read original CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# 2) Split by original rows (no expansion yet)\n",
    "train_df, val_df, test_df = split_dataframe_rows(df, TRAIN_FRAC, VAL_FRAC, TEST_FRAC, seed=SEED)\n",
    "print(f\"Row splits: train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\")\n",
    "\n",
    "# 3) Expand each split separately (no cross-split leakage)\n",
    "X_train, Y_train = expand_split(train_df)\n",
    "X_val,   Y_val   = expand_split(val_df)\n",
    "X_test,  Y_test  = expand_split(test_df)\n",
    "\n",
    "print(\"Expanded sample counts:\", f\"train={len(X_train)}, val={len(X_val)}, test={len(X_test)}\")\n",
    "print(\"Feature/Target dims:\", X_train.shape[1], Y_train.shape[1])\n",
    "\n",
    "# 4) Fit Min–Max on TRAIN only; transform all splits\n",
    "x_mins, x_rng = fit_minmax(X_train)\n",
    "y_mins, y_rng = fit_minmax(Y_train)\n",
    "\n",
    "X_train_n = transform_minmax(X_train, x_mins, x_rng)\n",
    "X_val_n   = transform_minmax(X_val,   x_mins, x_rng)\n",
    "X_test_n  = transform_minmax(X_test,  x_mins, x_rng)\n",
    "\n",
    "Y_train_n = transform_minmax(Y_train, y_mins, y_rng)\n",
    "Y_val_n   = transform_minmax(Y_val,   y_mins, y_rng)\n",
    "Y_test_n  = transform_minmax(Y_test,  y_mins, y_rng)\n",
    "\n",
    "# 5) Convert to torch tensors (ready for DataLoaders)\n",
    "X_train_t = torch.from_numpy(X_train_n)\n",
    "Y_train_t = torch.from_numpy(Y_train_n)\n",
    "X_val_t   = torch.from_numpy(X_val_n)\n",
    "Y_val_t   = torch.from_numpy(Y_val_n)\n",
    "X_test_t  = torch.from_numpy(X_test_n)\n",
    "Y_test_t  = torch.from_numpy(Y_test_n)\n",
    "\n",
    "print(\"Torch tensors:\",\n",
    "      X_train_t.shape, Y_train_t.shape,\n",
    "      X_val_t.shape,   Y_val_t.shape,\n",
    "      X_test_t.shape,  Y_test_t.shape)\n",
    "\n",
    "# Save normalization params for inference-time inverse-transform\n",
    "np.savez(\n",
    "    \"minmax_params_corrected.npz\",\n",
    "    x_mins=x_mins, x_rng=x_rng,\n",
    "    y_mins=y_mins, y_rng=y_rng\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e3237",
   "metadata": {},
   "source": [
    "### 2. \n",
    "Code for saving normalized datasets and scalers. Also example of how to load test dataset, unnormalize it and evaluate in trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0bebfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# (Optional) Save artifacts for later use\n",
    "# ---------------------------\n",
    "# Save scaling parameters (so you can inverse-transform predictions later)\n",
    "np.savez(\n",
    "    \"minmax_params.npz\",\n",
    "    x_mins=x_mins, x_rng=x_rng,\n",
    "    y_mins=y_mins, y_rng=y_rng\n",
    ")\n",
    "\n",
    "# Save splits (numpy or torch—choose one). Here: numpy.\n",
    "np.savez_compressed(\n",
    "    \"dataset_splits_minmax.npz\",\n",
    "    X_train=X_train_n, Y_train=Y_train_n,\n",
    "    X_val=X_val_n,     Y_val=Y_val_n,\n",
    "    X_test=X_test_n,   Y_test=Y_test_n,\n",
    "    groups=groups\n",
    ")\n",
    "\n",
    "# Example inverse-transform for predictions later:\n",
    "# with np.load(\"minmax_params.npz\") as d:\n",
    "#     y_mins, y_rng = d[\"y_mins\"], d[\"y_rng\"]\n",
    "# y_pred = model(X_test_t).detach().cpu().numpy()\n",
    "# y_pred_original_units = inverse_minmax(y_pred, y_mins, y_rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a2e0bc",
   "metadata": {},
   "source": [
    "### 3.\n",
    "Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8a728c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims, out_dim, activation=\"silu\", dropout=0.0):\n",
    "        super().__init__()\n",
    "        acts = {\n",
    "            \"relu\": nn.ReLU,\n",
    "            \"gelu\": nn.GELU,\n",
    "            \"silu\": nn.SiLU,\n",
    "            \"tanh\": nn.Tanh\n",
    "        }\n",
    "        if activation not in acts:\n",
    "            raise ValueError(f\"activation must be one of {list(acts.keys())}\")\n",
    "        Act = acts[activation]\n",
    "\n",
    "        layers = []\n",
    "        prev = in_dim\n",
    "        for i, h in enumerate(hidden_dims):\n",
    "            layers += [nn.Linear(prev, h), Act()]\n",
    "            if dropout and dropout > 0:\n",
    "                layers += [nn.Dropout(p=dropout)]\n",
    "            prev = h\n",
    "        # Linear output for regression\n",
    "        layers += [nn.Linear(prev, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        # Weight init (He for ReLU/SiLU/GELU, Xavier for Tanh)\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if activation in (\"relu\", \"silu\", \"gelu\"):\n",
    "                    nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "                else:  # tanh\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n",
    "                    bound = 1 / math.sqrt(fan_in)\n",
    "                    nn.init.uniform_(m.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "in_dim  = X_train_t.shape[1]\n",
    "out_dim = Y_train_t.shape[1]\n",
    "hidden  = [128, 128, 128]  # good starting point; try [128,128] if you want simpler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DNN(in_dim, hidden, out_dim, activation=\"silu\", dropout=0.0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58179d4",
   "metadata": {},
   "source": [
    "### 4.\n",
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65310536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iagr9\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_ds = TensorDataset(X_train_t, Y_train_t)\n",
    "val_ds   = TensorDataset(X_val_t,   Y_val_t)\n",
    "test_ds  = TensorDataset(X_test_t,  Y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "criterion = nn.MSELoss()  # targets are scaled; MSE is fine\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=10, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db6e47c",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e43372ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train MSE: 0.002052 | val MSE: 0.003406 | LR: 3.00e-03\n",
      "Epoch 002 | train MSE: 0.001976 | val MSE: 0.003209 | LR: 3.00e-03\n",
      "Epoch 003 | train MSE: 0.001805 | val MSE: 0.003627 | LR: 3.00e-03\n",
      "Epoch 004 | train MSE: 0.001846 | val MSE: 0.003233 | LR: 3.00e-03\n",
      "Epoch 005 | train MSE: 0.001912 | val MSE: 0.003325 | LR: 3.00e-03\n",
      "Epoch 006 | train MSE: 0.001815 | val MSE: 0.003280 | LR: 3.00e-03\n",
      "Epoch 007 | train MSE: 0.001788 | val MSE: 0.004895 | LR: 3.00e-03\n",
      "Epoch 008 | train MSE: 0.001789 | val MSE: 0.004135 | LR: 3.00e-03\n",
      "Epoch 009 | train MSE: 0.001795 | val MSE: 0.003182 | LR: 3.00e-03\n",
      "Epoch 010 | train MSE: 0.001750 | val MSE: 0.003257 | LR: 3.00e-03\n",
      "Epoch 011 | train MSE: 0.001637 | val MSE: 0.003771 | LR: 3.00e-03\n",
      "Epoch 012 | train MSE: 0.001744 | val MSE: 0.003650 | LR: 3.00e-03\n",
      "Epoch 013 | train MSE: 0.001793 | val MSE: 0.003903 | LR: 3.00e-03\n",
      "Epoch 014 | train MSE: 0.001632 | val MSE: 0.003709 | LR: 3.00e-03\n",
      "Epoch 015 | train MSE: 0.001674 | val MSE: 0.003311 | LR: 3.00e-03\n",
      "Epoch 016 | train MSE: 0.001715 | val MSE: 0.006841 | LR: 3.00e-03\n",
      "Epoch 017 | train MSE: 0.002058 | val MSE: 0.003132 | LR: 3.00e-03\n",
      "Epoch 018 | train MSE: 0.001538 | val MSE: 0.002877 | LR: 3.00e-03\n",
      "Epoch 019 | train MSE: 0.001504 | val MSE: 0.003531 | LR: 3.00e-03\n",
      "Epoch 020 | train MSE: 0.001466 | val MSE: 0.003592 | LR: 3.00e-03\n",
      "Epoch 021 | train MSE: 0.001536 | val MSE: 0.003288 | LR: 3.00e-03\n",
      "Epoch 022 | train MSE: 0.001507 | val MSE: 0.002941 | LR: 3.00e-03\n",
      "Epoch 023 | train MSE: 0.001538 | val MSE: 0.003305 | LR: 3.00e-03\n",
      "Epoch 024 | train MSE: 0.001457 | val MSE: 0.003321 | LR: 3.00e-03\n",
      "Epoch 025 | train MSE: 0.001464 | val MSE: 0.002841 | LR: 3.00e-03\n",
      "Epoch 026 | train MSE: 0.001400 | val MSE: 0.003006 | LR: 3.00e-03\n",
      "Epoch 027 | train MSE: 0.001500 | val MSE: 0.003287 | LR: 3.00e-03\n",
      "Epoch 028 | train MSE: 0.001404 | val MSE: 0.003117 | LR: 3.00e-03\n",
      "Epoch 029 | train MSE: 0.001475 | val MSE: 0.002914 | LR: 3.00e-03\n",
      "Epoch 030 | train MSE: 0.001427 | val MSE: 0.003359 | LR: 3.00e-03\n",
      "Epoch 031 | train MSE: 0.001402 | val MSE: 0.002869 | LR: 3.00e-03\n",
      "Epoch 032 | train MSE: 0.001334 | val MSE: 0.003355 | LR: 3.00e-03\n",
      "Epoch 033 | train MSE: 0.001328 | val MSE: 0.003293 | LR: 3.00e-03\n",
      "Epoch 034 | train MSE: 0.001340 | val MSE: 0.003151 | LR: 3.00e-03\n",
      "Epoch 035 | train MSE: 0.001283 | val MSE: 0.002744 | LR: 3.00e-03\n",
      "Epoch 036 | train MSE: 0.001337 | val MSE: 0.003227 | LR: 3.00e-03\n",
      "Epoch 037 | train MSE: 0.001313 | val MSE: 0.003958 | LR: 3.00e-03\n",
      "Epoch 038 | train MSE: 0.001556 | val MSE: 0.003017 | LR: 3.00e-03\n",
      "Epoch 039 | train MSE: 0.001290 | val MSE: 0.003517 | LR: 3.00e-03\n",
      "Epoch 040 | train MSE: 0.001353 | val MSE: 0.003310 | LR: 3.00e-03\n",
      "Epoch 041 | train MSE: 0.001284 | val MSE: 0.002626 | LR: 3.00e-03\n",
      "Epoch 042 | train MSE: 0.001448 | val MSE: 0.003195 | LR: 3.00e-03\n",
      "Epoch 043 | train MSE: 0.001300 | val MSE: 0.004632 | LR: 3.00e-03\n",
      "Epoch 044 | train MSE: 0.001927 | val MSE: 0.003291 | LR: 3.00e-03\n",
      "Epoch 045 | train MSE: 0.001410 | val MSE: 0.003241 | LR: 3.00e-03\n",
      "Epoch 046 | train MSE: 0.001304 | val MSE: 0.003405 | LR: 3.00e-03\n",
      "Epoch 047 | train MSE: 0.001353 | val MSE: 0.003069 | LR: 3.00e-03\n",
      "Epoch 048 | train MSE: 0.001355 | val MSE: 0.003353 | LR: 3.00e-03\n",
      "Epoch 049 | train MSE: 0.001203 | val MSE: 0.003315 | LR: 3.00e-03\n",
      "Epoch 050 | train MSE: 0.001250 | val MSE: 0.003270 | LR: 3.00e-03\n",
      "Epoch 051 | train MSE: 0.001224 | val MSE: 0.003404 | LR: 3.00e-03\n",
      "Epoch 052 | train MSE: 0.001229 | val MSE: 0.003435 | LR: 3.00e-03\n",
      "Epoch 053 | train MSE: 0.001087 | val MSE: 0.003280 | LR: 1.50e-03\n",
      "Epoch 054 | train MSE: 0.001069 | val MSE: 0.003387 | LR: 1.50e-03\n",
      "Epoch 055 | train MSE: 0.001025 | val MSE: 0.003426 | LR: 1.50e-03\n",
      "Epoch 056 | train MSE: 0.001021 | val MSE: 0.003502 | LR: 1.50e-03\n",
      "Epoch 057 | train MSE: 0.001006 | val MSE: 0.003681 | LR: 1.50e-03\n",
      "Epoch 058 | train MSE: 0.001078 | val MSE: 0.003926 | LR: 1.50e-03\n",
      "Epoch 059 | train MSE: 0.000996 | val MSE: 0.003540 | LR: 1.50e-03\n",
      "Epoch 060 | train MSE: 0.001030 | val MSE: 0.004208 | LR: 1.50e-03\n",
      "Epoch 061 | train MSE: 0.001020 | val MSE: 0.003763 | LR: 1.50e-03\n",
      "Epoch 062 | train MSE: 0.001038 | val MSE: 0.004316 | LR: 1.50e-03\n",
      "Epoch 063 | train MSE: 0.001026 | val MSE: 0.003971 | LR: 1.50e-03\n",
      "Epoch 064 | train MSE: 0.000930 | val MSE: 0.004146 | LR: 7.50e-04\n",
      "Epoch 065 | train MSE: 0.000900 | val MSE: 0.004171 | LR: 7.50e-04\n",
      "Epoch 066 | train MSE: 0.000907 | val MSE: 0.004377 | LR: 7.50e-04\n",
      "Epoch 067 | train MSE: 0.000888 | val MSE: 0.004237 | LR: 7.50e-04\n",
      "Epoch 068 | train MSE: 0.000906 | val MSE: 0.004373 | LR: 7.50e-04\n",
      "Epoch 069 | train MSE: 0.000892 | val MSE: 0.004676 | LR: 7.50e-04\n",
      "Epoch 070 | train MSE: 0.000893 | val MSE: 0.004521 | LR: 7.50e-04\n",
      "Epoch 071 | train MSE: 0.000886 | val MSE: 0.004670 | LR: 7.50e-04\n",
      "Epoch 072 | train MSE: 0.000877 | val MSE: 0.004549 | LR: 7.50e-04\n",
      "Epoch 073 | train MSE: 0.000881 | val MSE: 0.004563 | LR: 7.50e-04\n",
      "Epoch 074 | train MSE: 0.000870 | val MSE: 0.005012 | LR: 7.50e-04\n",
      "Epoch 075 | train MSE: 0.000830 | val MSE: 0.004912 | LR: 3.75e-04\n",
      "Epoch 076 | train MSE: 0.000829 | val MSE: 0.004860 | LR: 3.75e-04\n",
      "Epoch 077 | train MSE: 0.000826 | val MSE: 0.005008 | LR: 3.75e-04\n",
      "Epoch 078 | train MSE: 0.000820 | val MSE: 0.004934 | LR: 3.75e-04\n",
      "Epoch 079 | train MSE: 0.000819 | val MSE: 0.005246 | LR: 3.75e-04\n",
      "Epoch 080 | train MSE: 0.000815 | val MSE: 0.005028 | LR: 3.75e-04\n",
      "Epoch 081 | train MSE: 0.000814 | val MSE: 0.005354 | LR: 3.75e-04\n",
      "Epoch 082 | train MSE: 0.000812 | val MSE: 0.005033 | LR: 3.75e-04\n",
      "Epoch 083 | train MSE: 0.000818 | val MSE: 0.005088 | LR: 3.75e-04\n",
      "Epoch 084 | train MSE: 0.000804 | val MSE: 0.005005 | LR: 3.75e-04\n",
      "Epoch 085 | train MSE: 0.000806 | val MSE: 0.005217 | LR: 3.75e-04\n",
      "Epoch 086 | train MSE: 0.000786 | val MSE: 0.005319 | LR: 1.88e-04\n",
      "Epoch 087 | train MSE: 0.000789 | val MSE: 0.005331 | LR: 1.88e-04\n",
      "Epoch 088 | train MSE: 0.000789 | val MSE: 0.005291 | LR: 1.88e-04\n",
      "Epoch 089 | train MSE: 0.000780 | val MSE: 0.005497 | LR: 1.88e-04\n",
      "Epoch 090 | train MSE: 0.000777 | val MSE: 0.005462 | LR: 1.88e-04\n",
      "Epoch 091 | train MSE: 0.000777 | val MSE: 0.005383 | LR: 1.88e-04\n",
      "Early stopping at epoch 91. Best val MSE: 0.002626\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 300\n",
    "PATIENCE = 50\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "epochs_no_improve = 0\n",
    "losses_history = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # validate\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    losses_history.append([train_loss, val_loss, lr])\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val - 1e-8:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | train MSE: {train_loss:.6f} | val MSE: {val_loss:.6f} | LR: {lr:.2e}\")\n",
    "\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch}. Best val MSE: {best_val:.6f}\")\n",
    "        break\n",
    "\n",
    "\n",
    "# Load best model\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efe0c31",
   "metadata": {},
   "source": [
    "Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6428b386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE (scaled): 0.017979\n"
     ]
    }
   ],
   "source": [
    "def evaluate_mse(loader):\n",
    "    model.eval()\n",
    "    mse = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            pred = model(xb)\n",
    "            mse += nn.functional.mse_loss(pred, yb, reduction=\"sum\").item()\n",
    "    return mse / len(loader.dataset)\n",
    "\n",
    "test_mse = evaluate_mse(test_loader)\n",
    "print(f\"Test MSE (scaled): {test_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784fa630",
   "metadata": {},
   "source": [
    "See metrics in original units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6367a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE (original units) per target: [0.00091699 0.00367226 0.00178231]\n",
      "Test MAPE (original units) per target: [0.09169862 0.36722565 0.1782309 ] %\n",
      "Test RMSE (original units) per target: [0.00155602 0.00577832 0.00285016]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mm = np.load(\"minmax_params_corrected.npz\")\n",
    "    y_mins, y_rng = mm[\"y_mins\"], mm[\"y_rng\"]\n",
    "\n",
    "    # Compute MAE/RMSE in original units\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            pred = model(xb).cpu().numpy()\n",
    "            preds.append(pred)\n",
    "            trues.append(yb.numpy())\n",
    "    preds = np.vstack(preds)\n",
    "    trues = np.vstack(trues)\n",
    "\n",
    "    preds_u = inverse_minmax(preds, y_mins, y_rng)\n",
    "    trues_u = inverse_minmax(trues, y_mins, y_rng)\n",
    "\n",
    "    mae = np.mean(np.abs(preds_u - trues_u), axis=0)\n",
    "    mape = 100 * mae\n",
    "    rmse = np.sqrt(np.mean((preds_u - trues_u) ** 2, axis=0))\n",
    "    print(\"Test MAE (original units) per target:\", mae)\n",
    "    print(\"Test MAPE (original units) per target:\", mape, '%')\n",
    "    print(\"Test RMSE (original units) per target:\", rmse)\n",
    "except FileNotFoundError:\n",
    "    print(\"minmax_params.npz not found; skipping metrics in original units.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8aeccd",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19158c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to dnn_2.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"dnn_2.pt\")\n",
    "print(\"Saved model to dnn_2.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e516b",
   "metadata": {},
   "source": [
    "### 5.\n",
    "See results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a165bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iagr9\\AppData\\Local\\Temp\\ipykernel_104552\\3800467297.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"dnn_1.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions (10 rows, unnormalized/original units):\n",
      "[[0.99287397 0.03805349 0.10114258]\n",
      " [0.99290425 0.03797169 0.10083118]\n",
      " [0.9929335  0.03741462 0.10047552]\n",
      " [0.9929566  0.03622846 0.10010925]\n",
      " [0.99296784 0.03427822 0.09977768]\n",
      " [0.99296284 0.03149965 0.09953292]\n",
      " [0.99293995 0.02796858 0.09942468]\n",
      " [0.99290204 0.02396763 0.09948894]\n",
      " [0.9928567  0.02001463 0.09973994]\n",
      " [0.99281484 0.01681774 0.10017047]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Load saved model if needed\n",
    "hidden  = [128, 128, 128]  # good starting point; try [128,128] if you want simpler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "in_dim = 14\n",
    "out_dim = 3\n",
    "model = DNN(in_dim, hidden, out_dim, activation=\"silu\", dropout=0.0).to(device)\n",
    "model.load_state_dict(torch.load(\"dnn_1.pt\"))\n",
    "model.eval()  # set to evaluation mode\n",
    "\n",
    "\n",
    "dV_ges    = 1007         # original units (as in CSV)\n",
    "eps_0     = 0.4\n",
    "phi_0     = 551e-6\n",
    "h_dis_0   = 0.04\n",
    "h_c_0     = 0.1\n",
    "rho_c     = 1000\n",
    "rho_d     = 825\n",
    "eta_c     = 82e-5\n",
    "eta_d     = 604e-5\n",
    "sigma     = 0.00817\n",
    "T         = 30\n",
    "r_s_star  = 0.0085\n",
    "h_p_star  = 0.2211\n",
    "x_array   = np.linspace(0.25, 1.0, 10)  # DPZ_pos\n",
    "\n",
    "# Apply the SAME transformations you used when building X in training:\n",
    "# dV_ges -> /3.6 * 1e-6 ; DPZ_pos was in meters AFTER dividing by 100 in the CSV pipeline.\n",
    "# Here, x_array is already in meters, so no extra /100.\n",
    "dV_ges_tr = dV_ges / 3.6 * 1e-6\n",
    "\n",
    "const_feats = np.array(\n",
    "    [dV_ges_tr, eps_0, phi_0, h_dis_0, h_c_0,\n",
    "     rho_c, rho_d, eta_c, eta_d, sigma, T, r_s_star, h_p_star],\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "# Stack 10 rows, each with the 13 constant features + the varying DPZ_pos\n",
    "X_real = np.hstack([\n",
    "    np.repeat(const_feats[None, :], repeats=len(x_array), axis=0),\n",
    "    x_array.reshape(-1, 1).astype(np.float32)\n",
    "])  # shape (10, 14)\n",
    "\n",
    "assert X_real.shape[1] == in_dim, f\"Expected {in_dim} features, got {X_real.shape[1]}.\"\n",
    "\n",
    "X_scaled = transform_minmax(X_real, x_mins, x_rng)\n",
    "X_t = torch.from_numpy(X_scaled).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    Y_scaled = model(X_t).cpu().numpy()\n",
    "\n",
    "def transform_minmax(X, mins, rng):\n",
    "    rng_safe = np.where(rng > 0, rng, 1.0)\n",
    "    return (X - mins) / rng_safe\n",
    "\n",
    "def inverse_minmax(Ys, mins, rng):\n",
    "    return Ys * rng + mins\n",
    "\n",
    "Y_pred = inverse_minmax(Y_scaled, y_mins, y_rng)  \n",
    "\n",
    "print(\"Predictions (10 rows, unnormalized/original units):\")\n",
    "print(Y_pred)\n",
    "\n",
    "plt.plot(x_array, Y_pred[:, 2])\n",
    "plt.plot(x_array, Y_pred[:,1]+Y_pred[:,2], label='DPZ_top')\n",
    "plt.title(f'Separation eff. {Y_pred[-1,0]}')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
