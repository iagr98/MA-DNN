{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2afbd7f7",
   "metadata": {},
   "source": [
    "### 1. \n",
    "Import data, split, normalize and save. (Only run once!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f56578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def fit_minmax(X):\n",
    "    \"\"\"Fit per-feature min-max on X (2D). Returns (mins, ranges) with safe ranges.\"\"\"\n",
    "    mins = X.min(axis=0)\n",
    "    maxs = X.max(axis=0)\n",
    "    rng = maxs - mins\n",
    "    rng_safe = np.where(rng > 0, rng, 1.0)  # avoid division by zero (constant features)\n",
    "    return mins.astype(DTYPE), rng_safe.astype(DTYPE)\n",
    "\n",
    "def transform_minmax(X, mins, rng_safe):\n",
    "    return ((X - mins) / rng_safe).astype(DTYPE)\n",
    "\n",
    "def inverse_minmax(X_scaled, mins, rng_safe):\n",
    "    return (X_scaled * rng_safe + mins).astype(DTYPE)\n",
    "\n",
    "def split_dataframe_rows(df, train_frac, val_frac, test_frac, seed=42):\n",
    "    \"\"\"Split the *rows* of the original df into train/val/test by fraction.\"\"\"\n",
    "    assert abs((train_frac + val_frac + test_frac) - 1.0) < 1e-8, \"Fractions must sum to 1.\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = df.index.to_numpy()\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    n = len(idx)\n",
    "    n_train = int(round(train_frac * n))\n",
    "    n_val = int(round(val_frac * n))\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train_idx = idx[:n_train]\n",
    "    val_idx   = idx[n_train:n_train + n_val]\n",
    "    test_idx  = idx[n_train + n_val:]\n",
    "\n",
    "    return df.loc[train_idx], df.loc[val_idx], df.loc[test_idx]\n",
    "\n",
    "\n",
    "def create_inp_out(df_split):\n",
    "    \"\"\"\n",
    "    Creates inputs X and outputs Y of given df already splitted\n",
    "    \"\"\"\n",
    "    inputs, outputs = [], []\n",
    "\n",
    "    for _, row in df_split.iterrows():\n",
    "        # ---- scalar inputs (with the same preprocessing as in your pipeline) ----\n",
    "        dV_ges   = float(row[\"dV_ges\"]) / 3.6 * 1e-6\n",
    "        eps_0    = float(row[\"eps_0\"])\n",
    "        phi_0    = float(row[\"phi_0\"])\n",
    "        h_dis_0  = float(row[\"h_dis_0\"])\n",
    "        h_c_0    = float(row[\"h_c_0\"])\n",
    "        rho_c    = float(row[\"rho_c\"])\n",
    "        rho_d    = float(row[\"rho_d\"])\n",
    "        eta_c    = float(row[\"eta_c\"])\n",
    "        eta_d    = float(row[\"etc_d\"])\n",
    "        sigma    = float(row[\"sigma\"])\n",
    "        T        = float(row[\"T\"])\n",
    "        r_s_star = float(row[\"r_S_star\"])\n",
    "        h_p_star = float(row[\"h_p_star\"])\n",
    "        D_A      = float(row[\"D_A\"])\n",
    "        L_A      = float(row[\"L_A\"])\n",
    "        lam      = float(row[\"lambda\"])\n",
    "\n",
    "        # ---- expand: one sample per DPZ position ----\n",
    "        inputs.append([dV_ges, eps_0, phi_0, h_dis_0, h_c_0, rho_c, rho_d,\n",
    "                       eta_c, eta_d, sigma, T, r_s_star, h_p_star, D_A, L_A])\n",
    "        outputs.append(lam)\n",
    "\n",
    "    X = np.array(inputs, dtype=DTYPE)\n",
    "    Y = np.array(outputs, dtype=DTYPE).reshape(-1,1)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c62fbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row splits: train=276, val=59, test=60\n",
      "Sample counts: train=276, val=59, test=60\n",
      "Feature/Target dims: 15 1\n",
      "Torch tensors: torch.Size([276, 15]) torch.Size([276, 1]) torch.Size([59, 15]) torch.Size([59, 1]) torch.Size([60, 15]) torch.Size([60, 1])\n"
     ]
    }
   ],
   "source": [
    "TRAIN_FRAC, VAL_FRAC, TEST_FRAC = 0.70, 0.15, 0.15\n",
    "SEED = 42\n",
    "DTYPE = np.float32\n",
    "CSV_PATH = os.path.join(\"Input\", \"df_lam.csv\")\n",
    "\n",
    "# 1) Read original CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# 2) Split by original rows\n",
    "train_df, val_df, test_df = split_dataframe_rows(df, TRAIN_FRAC, VAL_FRAC, TEST_FRAC, seed=SEED)\n",
    "print(f\"Row splits: train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\")\n",
    "test_df.to_csv(\"df_lam_te.csv\", index=False)\n",
    "\n",
    "# 3) Expand each split separately (no cross-split leakage)\n",
    "X_train, Y_train = create_inp_out(train_df)\n",
    "X_val,   Y_val   = create_inp_out(val_df)\n",
    "X_test,  Y_test  = create_inp_out(test_df)\n",
    "\n",
    "print(\"Sample counts:\", f\"train={len(X_train)}, val={len(X_val)}, test={len(X_test)}\")\n",
    "print(\"Feature/Target dims:\", X_train.shape[1], Y_train.shape[1])\n",
    "\n",
    "# 4) Fit Minâ€“Max on TRAIN only; transform all splits\n",
    "x_mins, x_rng = fit_minmax(X_train)\n",
    "y_mins, y_rng = fit_minmax(Y_train)\n",
    "\n",
    "X_train_n = transform_minmax(X_train, x_mins, x_rng)\n",
    "X_val_n   = transform_minmax(X_val,   x_mins, x_rng)\n",
    "X_test_n  = transform_minmax(X_test,  x_mins, x_rng)\n",
    "\n",
    "Y_train_n = transform_minmax(Y_train, y_mins, y_rng)\n",
    "Y_val_n   = transform_minmax(Y_val,   y_mins, y_rng)\n",
    "Y_test_n  = transform_minmax(Y_test,  y_mins, y_rng)\n",
    "\n",
    "# 5) Convert to torch tensors (ready for DataLoaders)\n",
    "X_train_t = torch.from_numpy(X_train_n)\n",
    "Y_train_t = torch.from_numpy(Y_train_n)\n",
    "X_val_t   = torch.from_numpy(X_val_n)\n",
    "Y_val_t   = torch.from_numpy(Y_val_n)\n",
    "X_test_t  = torch.from_numpy(X_test_n)\n",
    "Y_test_t  = torch.from_numpy(Y_test_n)\n",
    "\n",
    "print(\"Torch tensors:\",\n",
    "      X_train_t.shape, Y_train_t.shape,\n",
    "      X_val_t.shape,   Y_val_t.shape,\n",
    "      X_test_t.shape,  Y_test_t.shape)\n",
    "\n",
    "# Save normalization params for inference-time inverse-transform\n",
    "np.savez(\n",
    "    \"minmax_params_dnn_7_(lam).npz\",\n",
    "    x_mins=x_mins, x_rng=x_rng,\n",
    "    y_mins=y_mins, y_rng=y_rng\n",
    ")\n",
    "\n",
    "# Save splits as torch tensors\n",
    "data = {\n",
    "    \"X_train_t\": X_train_t,\n",
    "    \"Y_train_t\": Y_train_t,\n",
    "    \"X_val_t\":   X_val_t,\n",
    "    \"Y_val_t\":   Y_val_t,\n",
    "    \"X_test_t\":  X_test_t,\n",
    "    \"Y_test_t\":  Y_test_t,\n",
    "    \"X_train_n\": X_train_n,\n",
    "    \"Y_train_n\": Y_train_n,\n",
    "    \"X_val_n\":   X_val_n,\n",
    "    \"Y_val_n\":   Y_val_n,\n",
    "    \"X_test_n\":  X_test_n,\n",
    "    \"Y_test_n\":  Y_test_n,\n",
    "}\n",
    "\n",
    "# Save all in one file\n",
    "torch.save(data, 'datasets_dnn_7_(lam).pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8cade",
   "metadata": {},
   "source": [
    "### 2. \n",
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5279de1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_15536\\2158836666.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_data = torch.load(\"saved_models/dnn_7_(lam)/datasets_dnn_7_(lam).pt\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DTYPE = np.float32\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True  # faster on GPU\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "loaded_data = torch.load(\"saved_models/dnn_7_(lam)/datasets_dnn_7_(lam).pt\")\n",
    "\n",
    "# Access tensors or arrays\n",
    "X_train_t = loaded_data[\"X_train_t\"]\n",
    "X_train_n = loaded_data[\"X_train_n\"]\n",
    "Y_train_t = loaded_data[\"Y_train_t\"]\n",
    "Y_train_n = loaded_data[\"Y_train_n\"]\n",
    "X_val_t = loaded_data[\"X_val_t\"]\n",
    "X_val_n = loaded_data[\"X_val_n\"]\n",
    "Y_val_t = loaded_data[\"Y_val_t\"]\n",
    "Y_val_n = loaded_data[\"Y_val_n\"]\n",
    "X_test_t = loaded_data[\"X_test_t\"]\n",
    "X_test_n = loaded_data[\"X_test_n\"]\n",
    "Y_test_t = loaded_data[\"Y_test_t\"]\n",
    "Y_test_n = loaded_data[\"Y_test_n\"]\n",
    "\n",
    "loaded_data_n = np.load(\"saved_models/dnn_7_(lam)/minmax_params_dnn_7_(lam).npz\")\n",
    "x_mins = loaded_data_n[\"x_mins\"]\n",
    "x_rng = loaded_data_n[\"x_rng\"]\n",
    "y_mins = loaded_data_n[\"y_mins\"]\n",
    "y_rng = loaded_data_n[\"y_rng\"]\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims, out_dim, activation=\"silu\", dropout=0.0):\n",
    "        super().__init__()\n",
    "        acts = {\n",
    "            \"relu\": nn.ReLU,\n",
    "            \"gelu\": nn.GELU,\n",
    "            \"silu\": nn.SiLU,\n",
    "            \"tanh\": nn.Tanh\n",
    "        }\n",
    "        if activation not in acts:\n",
    "            raise ValueError(f\"activation must be one of {list(acts.keys())}\")\n",
    "        Act = acts[activation]\n",
    "\n",
    "        layers = []\n",
    "        prev = in_dim\n",
    "        for i, h in enumerate(hidden_dims):\n",
    "            layers += [nn.Linear(prev, h), Act()]\n",
    "            if dropout and dropout > 0:\n",
    "                layers += [nn.Dropout(p=dropout)]\n",
    "            prev = h\n",
    "        # Linear output for regression\n",
    "        layers += [nn.Linear(prev, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        # Weight init (He for ReLU/SiLU/GELU, Xavier for Tanh)\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if activation in (\"relu\", \"silu\", \"gelu\"):\n",
    "                    nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "                else:  # tanh\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n",
    "                    bound = 1 / math.sqrt(fan_in)\n",
    "                    nn.init.uniform_(m.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "in_dim  = X_train_t.shape[1]\n",
    "out_dim = Y_train_t.shape[1]\n",
    "hidden  = [192, 64, 96]\n",
    "dropout = 0.0357\n",
    "activation = 'gelu'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DNN(in_dim, hidden, out_dim, activation=activation, dropout=dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d473e810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_ds = TensorDataset(X_train_t, Y_train_t)\n",
    "val_ds   = TensorDataset(X_val_t,   Y_val_t)\n",
    "test_ds  = TensorDataset(X_test_t,  Y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "criterion = nn.MSELoss()  # targets are scaled; MSE is fine\n",
    "lr = 2.2e-3\n",
    "weight_decay = 1.62e-6\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7927a992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train MSE: 0.895600 | val MSE: 0.325990 | LR: 2.20e-03\n",
      "Epoch 002 | train MSE: 0.155162 | val MSE: 0.189718 | LR: 2.20e-03\n",
      "Epoch 003 | train MSE: 0.082635 | val MSE: 0.042945 | LR: 2.20e-03\n",
      "Epoch 004 | train MSE: 0.068131 | val MSE: 0.046153 | LR: 2.20e-03\n",
      "Epoch 005 | train MSE: 0.028451 | val MSE: 0.007058 | LR: 2.20e-03\n",
      "Epoch 006 | train MSE: 0.021761 | val MSE: 0.015547 | LR: 2.20e-03\n",
      "Epoch 007 | train MSE: 0.014265 | val MSE: 0.005434 | LR: 2.20e-03\n",
      "Epoch 008 | train MSE: 0.013311 | val MSE: 0.007691 | LR: 2.20e-03\n",
      "Epoch 009 | train MSE: 0.009564 | val MSE: 0.001882 | LR: 2.20e-03\n",
      "Epoch 010 | train MSE: 0.007721 | val MSE: 0.003274 | LR: 2.20e-03\n",
      "Epoch 011 | train MSE: 0.008618 | val MSE: 0.001816 | LR: 2.20e-03\n",
      "Epoch 012 | train MSE: 0.007942 | val MSE: 0.002580 | LR: 2.20e-03\n",
      "Epoch 013 | train MSE: 0.007559 | val MSE: 0.001443 | LR: 2.20e-03\n",
      "Epoch 014 | train MSE: 0.007635 | val MSE: 0.001517 | LR: 2.20e-03\n",
      "Epoch 015 | train MSE: 0.007250 | val MSE: 0.001541 | LR: 2.20e-03\n",
      "Epoch 016 | train MSE: 0.007239 | val MSE: 0.001448 | LR: 2.20e-03\n",
      "Epoch 017 | train MSE: 0.006298 | val MSE: 0.001273 | LR: 2.20e-03\n",
      "Epoch 018 | train MSE: 0.006118 | val MSE: 0.001228 | LR: 2.20e-03\n",
      "Epoch 019 | train MSE: 0.007014 | val MSE: 0.001353 | LR: 2.20e-03\n",
      "Epoch 020 | train MSE: 0.005963 | val MSE: 0.001182 | LR: 2.20e-03\n",
      "Epoch 021 | train MSE: 0.006327 | val MSE: 0.001156 | LR: 2.20e-03\n",
      "Epoch 022 | train MSE: 0.006955 | val MSE: 0.001189 | LR: 2.20e-03\n",
      "Epoch 023 | train MSE: 0.006722 | val MSE: 0.001183 | LR: 2.20e-03\n",
      "Epoch 024 | train MSE: 0.006364 | val MSE: 0.001176 | LR: 2.20e-03\n",
      "Epoch 025 | train MSE: 0.005761 | val MSE: 0.001133 | LR: 2.20e-03\n",
      "Epoch 026 | train MSE: 0.006654 | val MSE: 0.001105 | LR: 2.20e-03\n",
      "Epoch 027 | train MSE: 0.006563 | val MSE: 0.001056 | LR: 2.20e-03\n",
      "Epoch 028 | train MSE: 0.006658 | val MSE: 0.001031 | LR: 2.20e-03\n",
      "Epoch 029 | train MSE: 0.007147 | val MSE: 0.001014 | LR: 2.20e-03\n",
      "Epoch 030 | train MSE: 0.005984 | val MSE: 0.001144 | LR: 2.20e-03\n",
      "Epoch 031 | train MSE: 0.006335 | val MSE: 0.001103 | LR: 2.20e-03\n",
      "Epoch 032 | train MSE: 0.006119 | val MSE: 0.001041 | LR: 2.20e-03\n",
      "Epoch 033 | train MSE: 0.006063 | val MSE: 0.001058 | LR: 2.20e-03\n",
      "Epoch 034 | train MSE: 0.005934 | val MSE: 0.001039 | LR: 2.20e-03\n",
      "Epoch 035 | train MSE: 0.006476 | val MSE: 0.001145 | LR: 2.20e-03\n",
      "Epoch 036 | train MSE: 0.006709 | val MSE: 0.001223 | LR: 2.20e-03\n",
      "Epoch 037 | train MSE: 0.006883 | val MSE: 0.001060 | LR: 2.20e-03\n",
      "Epoch 038 | train MSE: 0.006368 | val MSE: 0.001052 | LR: 2.20e-03\n",
      "Epoch 039 | train MSE: 0.006025 | val MSE: 0.000960 | LR: 2.20e-03\n",
      "Epoch 040 | train MSE: 0.005754 | val MSE: 0.000965 | LR: 2.20e-03\n",
      "Epoch 041 | train MSE: 0.006003 | val MSE: 0.000909 | LR: 2.20e-03\n",
      "Epoch 042 | train MSE: 0.006251 | val MSE: 0.000962 | LR: 2.20e-03\n",
      "Epoch 043 | train MSE: 0.005877 | val MSE: 0.000922 | LR: 2.20e-03\n",
      "Epoch 044 | train MSE: 0.005860 | val MSE: 0.000967 | LR: 2.20e-03\n",
      "Epoch 045 | train MSE: 0.006099 | val MSE: 0.001005 | LR: 2.20e-03\n",
      "Epoch 046 | train MSE: 0.006365 | val MSE: 0.000993 | LR: 2.20e-03\n",
      "Epoch 047 | train MSE: 0.005718 | val MSE: 0.001012 | LR: 2.20e-03\n",
      "Epoch 048 | train MSE: 0.006037 | val MSE: 0.000990 | LR: 2.20e-03\n",
      "Epoch 049 | train MSE: 0.005760 | val MSE: 0.000997 | LR: 2.20e-03\n",
      "Epoch 050 | train MSE: 0.006192 | val MSE: 0.000885 | LR: 2.20e-03\n",
      "Epoch 051 | train MSE: 0.005428 | val MSE: 0.000913 | LR: 2.20e-03\n",
      "Epoch 052 | train MSE: 0.005674 | val MSE: 0.000909 | LR: 2.20e-03\n",
      "Epoch 053 | train MSE: 0.006024 | val MSE: 0.000888 | LR: 2.20e-03\n",
      "Epoch 054 | train MSE: 0.005774 | val MSE: 0.001021 | LR: 2.20e-03\n",
      "Epoch 055 | train MSE: 0.005849 | val MSE: 0.000899 | LR: 2.20e-03\n",
      "Epoch 056 | train MSE: 0.006408 | val MSE: 0.000860 | LR: 2.20e-03\n",
      "Epoch 057 | train MSE: 0.005987 | val MSE: 0.000917 | LR: 2.20e-03\n",
      "Epoch 058 | train MSE: 0.005953 | val MSE: 0.000848 | LR: 2.20e-03\n",
      "Epoch 059 | train MSE: 0.006178 | val MSE: 0.000858 | LR: 2.20e-03\n",
      "Epoch 060 | train MSE: 0.005984 | val MSE: 0.000928 | LR: 2.20e-03\n",
      "Epoch 061 | train MSE: 0.005764 | val MSE: 0.000865 | LR: 2.20e-03\n",
      "Epoch 062 | train MSE: 0.006042 | val MSE: 0.000850 | LR: 2.20e-03\n",
      "Epoch 063 | train MSE: 0.005799 | val MSE: 0.000988 | LR: 2.20e-03\n",
      "Epoch 064 | train MSE: 0.006029 | val MSE: 0.000953 | LR: 2.20e-03\n",
      "Epoch 065 | train MSE: 0.005853 | val MSE: 0.000956 | LR: 2.20e-03\n",
      "Epoch 066 | train MSE: 0.006007 | val MSE: 0.000930 | LR: 2.20e-03\n",
      "Epoch 067 | train MSE: 0.005850 | val MSE: 0.000925 | LR: 2.20e-03\n",
      "Epoch 068 | train MSE: 0.006042 | val MSE: 0.000888 | LR: 2.20e-03\n",
      "Epoch 069 | train MSE: 0.005564 | val MSE: 0.000869 | LR: 2.20e-03\n",
      "Epoch 070 | train MSE: 0.005205 | val MSE: 0.000861 | LR: 2.20e-03\n",
      "Epoch 071 | train MSE: 0.005932 | val MSE: 0.000874 | LR: 2.20e-03\n",
      "Epoch 072 | train MSE: 0.005836 | val MSE: 0.000862 | LR: 2.20e-03\n",
      "Epoch 073 | train MSE: 0.006073 | val MSE: 0.000831 | LR: 2.20e-03\n",
      "Epoch 074 | train MSE: 0.005062 | val MSE: 0.000985 | LR: 2.20e-03\n",
      "Epoch 075 | train MSE: 0.005574 | val MSE: 0.000823 | LR: 2.20e-03\n",
      "Epoch 076 | train MSE: 0.005699 | val MSE: 0.000858 | LR: 2.20e-03\n",
      "Epoch 077 | train MSE: 0.005572 | val MSE: 0.000840 | LR: 2.20e-03\n",
      "Epoch 078 | train MSE: 0.005774 | val MSE: 0.000862 | LR: 2.20e-03\n",
      "Epoch 079 | train MSE: 0.005625 | val MSE: 0.000899 | LR: 2.20e-03\n",
      "Epoch 080 | train MSE: 0.005723 | val MSE: 0.000925 | LR: 2.20e-03\n",
      "Epoch 081 | train MSE: 0.006007 | val MSE: 0.000889 | LR: 2.20e-03\n",
      "Epoch 082 | train MSE: 0.005383 | val MSE: 0.000911 | LR: 2.20e-03\n",
      "Epoch 083 | train MSE: 0.005642 | val MSE: 0.000930 | LR: 2.20e-03\n",
      "Epoch 084 | train MSE: 0.005760 | val MSE: 0.000827 | LR: 2.20e-03\n",
      "Epoch 085 | train MSE: 0.005327 | val MSE: 0.000892 | LR: 2.20e-03\n",
      "Epoch 086 | train MSE: 0.005643 | val MSE: 0.000815 | LR: 2.20e-03\n",
      "Epoch 087 | train MSE: 0.005685 | val MSE: 0.000833 | LR: 2.20e-03\n",
      "Epoch 088 | train MSE: 0.005919 | val MSE: 0.000864 | LR: 2.20e-03\n",
      "Epoch 089 | train MSE: 0.005808 | val MSE: 0.000810 | LR: 2.20e-03\n",
      "Epoch 090 | train MSE: 0.005764 | val MSE: 0.000816 | LR: 2.20e-03\n",
      "Epoch 091 | train MSE: 0.005697 | val MSE: 0.000806 | LR: 2.20e-03\n",
      "Epoch 092 | train MSE: 0.005491 | val MSE: 0.000821 | LR: 2.20e-03\n",
      "Epoch 093 | train MSE: 0.005317 | val MSE: 0.000833 | LR: 2.20e-03\n",
      "Epoch 094 | train MSE: 0.005162 | val MSE: 0.000844 | LR: 2.20e-03\n",
      "Epoch 095 | train MSE: 0.005221 | val MSE: 0.000858 | LR: 2.20e-03\n",
      "Epoch 096 | train MSE: 0.005751 | val MSE: 0.000854 | LR: 2.20e-03\n",
      "Epoch 097 | train MSE: 0.005485 | val MSE: 0.001065 | LR: 2.20e-03\n",
      "Epoch 098 | train MSE: 0.004911 | val MSE: 0.001001 | LR: 2.20e-03\n",
      "Epoch 099 | train MSE: 0.005286 | val MSE: 0.000934 | LR: 2.20e-03\n",
      "Epoch 100 | train MSE: 0.005864 | val MSE: 0.000857 | LR: 2.20e-03\n",
      "Epoch 101 | train MSE: 0.005492 | val MSE: 0.000961 | LR: 2.20e-03\n",
      "Epoch 102 | train MSE: 0.005377 | val MSE: 0.000921 | LR: 2.20e-03\n",
      "Epoch 103 | train MSE: 0.005429 | val MSE: 0.000921 | LR: 2.20e-03\n",
      "Epoch 104 | train MSE: 0.005184 | val MSE: 0.000933 | LR: 2.20e-03\n",
      "Epoch 105 | train MSE: 0.005750 | val MSE: 0.000881 | LR: 2.20e-03\n",
      "Epoch 106 | train MSE: 0.005349 | val MSE: 0.000800 | LR: 2.20e-03\n",
      "Epoch 107 | train MSE: 0.005116 | val MSE: 0.000805 | LR: 2.20e-03\n",
      "Epoch 108 | train MSE: 0.005586 | val MSE: 0.000791 | LR: 2.20e-03\n",
      "Epoch 109 | train MSE: 0.005524 | val MSE: 0.000739 | LR: 2.20e-03\n",
      "Epoch 110 | train MSE: 0.005175 | val MSE: 0.000933 | LR: 2.20e-03\n",
      "Epoch 111 | train MSE: 0.005466 | val MSE: 0.000946 | LR: 2.20e-03\n",
      "Epoch 112 | train MSE: 0.005300 | val MSE: 0.001007 | LR: 2.20e-03\n",
      "Epoch 113 | train MSE: 0.005574 | val MSE: 0.000916 | LR: 2.20e-03\n",
      "Epoch 114 | train MSE: 0.005486 | val MSE: 0.000848 | LR: 2.20e-03\n",
      "Epoch 115 | train MSE: 0.005654 | val MSE: 0.000751 | LR: 2.20e-03\n",
      "Epoch 116 | train MSE: 0.005551 | val MSE: 0.000819 | LR: 2.20e-03\n",
      "Epoch 117 | train MSE: 0.005433 | val MSE: 0.000992 | LR: 2.20e-03\n",
      "Epoch 118 | train MSE: 0.005249 | val MSE: 0.000927 | LR: 2.20e-03\n",
      "Epoch 119 | train MSE: 0.005316 | val MSE: 0.000763 | LR: 2.20e-03\n",
      "Epoch 120 | train MSE: 0.004886 | val MSE: 0.000752 | LR: 2.20e-03\n",
      "Epoch 121 | train MSE: 0.004860 | val MSE: 0.000878 | LR: 2.20e-03\n",
      "Epoch 122 | train MSE: 0.005461 | val MSE: 0.000821 | LR: 2.20e-03\n",
      "Epoch 123 | train MSE: 0.005398 | val MSE: 0.000941 | LR: 2.20e-03\n",
      "Epoch 124 | train MSE: 0.006108 | val MSE: 0.001049 | LR: 2.20e-03\n",
      "Epoch 125 | train MSE: 0.006058 | val MSE: 0.000698 | LR: 2.20e-03\n",
      "Epoch 126 | train MSE: 0.005653 | val MSE: 0.000810 | LR: 2.20e-03\n",
      "Epoch 127 | train MSE: 0.005688 | val MSE: 0.000730 | LR: 2.20e-03\n",
      "Epoch 128 | train MSE: 0.005237 | val MSE: 0.000762 | LR: 2.20e-03\n",
      "Epoch 129 | train MSE: 0.005178 | val MSE: 0.000888 | LR: 2.20e-03\n",
      "Epoch 130 | train MSE: 0.005728 | val MSE: 0.001019 | LR: 2.20e-03\n",
      "Epoch 131 | train MSE: 0.005282 | val MSE: 0.000701 | LR: 2.20e-03\n",
      "Epoch 132 | train MSE: 0.005022 | val MSE: 0.000683 | LR: 2.20e-03\n",
      "Epoch 133 | train MSE: 0.005714 | val MSE: 0.000729 | LR: 2.20e-03\n",
      "Epoch 134 | train MSE: 0.005329 | val MSE: 0.000695 | LR: 2.20e-03\n",
      "Epoch 135 | train MSE: 0.005270 | val MSE: 0.000710 | LR: 2.20e-03\n",
      "Epoch 136 | train MSE: 0.005059 | val MSE: 0.000756 | LR: 2.20e-03\n",
      "Epoch 137 | train MSE: 0.005072 | val MSE: 0.000733 | LR: 2.20e-03\n",
      "Epoch 138 | train MSE: 0.005230 | val MSE: 0.000691 | LR: 2.20e-03\n",
      "Epoch 139 | train MSE: 0.005383 | val MSE: 0.000668 | LR: 2.20e-03\n",
      "Epoch 140 | train MSE: 0.005212 | val MSE: 0.000733 | LR: 2.20e-03\n",
      "Epoch 141 | train MSE: 0.005089 | val MSE: 0.000717 | LR: 2.20e-03\n",
      "Epoch 142 | train MSE: 0.004572 | val MSE: 0.000675 | LR: 2.20e-03\n",
      "Epoch 143 | train MSE: 0.005543 | val MSE: 0.000652 | LR: 2.20e-03\n",
      "Epoch 144 | train MSE: 0.005081 | val MSE: 0.000674 | LR: 2.20e-03\n",
      "Epoch 145 | train MSE: 0.005067 | val MSE: 0.000941 | LR: 2.20e-03\n",
      "Epoch 146 | train MSE: 0.004795 | val MSE: 0.000902 | LR: 2.20e-03\n",
      "Epoch 147 | train MSE: 0.005077 | val MSE: 0.000673 | LR: 2.20e-03\n",
      "Epoch 148 | train MSE: 0.004886 | val MSE: 0.000669 | LR: 2.20e-03\n",
      "Epoch 149 | train MSE: 0.004665 | val MSE: 0.000632 | LR: 2.20e-03\n",
      "Epoch 150 | train MSE: 0.005251 | val MSE: 0.000611 | LR: 2.20e-03\n",
      "Epoch 151 | train MSE: 0.004796 | val MSE: 0.000606 | LR: 2.20e-03\n",
      "Epoch 152 | train MSE: 0.004732 | val MSE: 0.000629 | LR: 2.20e-03\n",
      "Epoch 153 | train MSE: 0.004916 | val MSE: 0.000610 | LR: 2.20e-03\n",
      "Epoch 154 | train MSE: 0.005213 | val MSE: 0.000625 | LR: 2.20e-03\n",
      "Epoch 155 | train MSE: 0.004835 | val MSE: 0.000614 | LR: 2.20e-03\n",
      "Epoch 156 | train MSE: 0.004303 | val MSE: 0.001093 | LR: 2.20e-03\n",
      "Epoch 157 | train MSE: 0.005140 | val MSE: 0.000788 | LR: 2.20e-03\n",
      "Epoch 158 | train MSE: 0.004574 | val MSE: 0.000558 | LR: 2.20e-03\n",
      "Epoch 159 | train MSE: 0.004955 | val MSE: 0.000541 | LR: 2.20e-03\n",
      "Epoch 160 | train MSE: 0.004572 | val MSE: 0.000703 | LR: 2.20e-03\n",
      "Epoch 161 | train MSE: 0.004830 | val MSE: 0.000873 | LR: 2.20e-03\n",
      "Epoch 162 | train MSE: 0.005085 | val MSE: 0.000929 | LR: 2.20e-03\n",
      "Epoch 163 | train MSE: 0.005151 | val MSE: 0.000843 | LR: 2.20e-03\n",
      "Epoch 164 | train MSE: 0.004574 | val MSE: 0.000583 | LR: 2.20e-03\n",
      "Epoch 165 | train MSE: 0.004692 | val MSE: 0.001471 | LR: 2.20e-03\n",
      "Epoch 166 | train MSE: 0.005015 | val MSE: 0.001553 | LR: 2.20e-03\n",
      "Epoch 167 | train MSE: 0.004829 | val MSE: 0.000766 | LR: 2.20e-03\n",
      "Epoch 168 | train MSE: 0.004666 | val MSE: 0.000582 | LR: 2.20e-03\n",
      "Epoch 169 | train MSE: 0.004679 | val MSE: 0.000766 | LR: 2.20e-03\n",
      "Epoch 170 | train MSE: 0.004890 | val MSE: 0.000554 | LR: 2.20e-03\n",
      "Epoch 171 | train MSE: 0.004491 | val MSE: 0.000655 | LR: 2.20e-03\n",
      "Epoch 172 | train MSE: 0.004428 | val MSE: 0.000623 | LR: 2.20e-03\n",
      "Epoch 173 | train MSE: 0.004683 | val MSE: 0.000627 | LR: 2.20e-03\n",
      "Epoch 174 | train MSE: 0.004349 | val MSE: 0.000515 | LR: 2.20e-03\n",
      "Epoch 175 | train MSE: 0.004084 | val MSE: 0.000596 | LR: 2.20e-03\n",
      "Epoch 176 | train MSE: 0.004635 | val MSE: 0.000513 | LR: 2.20e-03\n",
      "Epoch 177 | train MSE: 0.004107 | val MSE: 0.000655 | LR: 2.20e-03\n",
      "Epoch 178 | train MSE: 0.004740 | val MSE: 0.000844 | LR: 2.20e-03\n",
      "Epoch 179 | train MSE: 0.007031 | val MSE: 0.000703 | LR: 2.20e-03\n",
      "Epoch 180 | train MSE: 0.005721 | val MSE: 0.001755 | LR: 2.20e-03\n",
      "Epoch 181 | train MSE: 0.006370 | val MSE: 0.000948 | LR: 2.20e-03\n",
      "Epoch 182 | train MSE: 0.005148 | val MSE: 0.000919 | LR: 2.20e-03\n",
      "Epoch 183 | train MSE: 0.004857 | val MSE: 0.002905 | LR: 2.20e-03\n",
      "Epoch 184 | train MSE: 0.005155 | val MSE: 0.000785 | LR: 2.20e-03\n",
      "Epoch 185 | train MSE: 0.004482 | val MSE: 0.000616 | LR: 2.20e-03\n",
      "Epoch 186 | train MSE: 0.004865 | val MSE: 0.000559 | LR: 2.20e-03\n",
      "Epoch 187 | train MSE: 0.004800 | val MSE: 0.000664 | LR: 2.20e-03\n",
      "Epoch 188 | train MSE: 0.004948 | val MSE: 0.001079 | LR: 2.20e-03\n",
      "Epoch 189 | train MSE: 0.004385 | val MSE: 0.000801 | LR: 2.20e-03\n",
      "Epoch 190 | train MSE: 0.005029 | val MSE: 0.000571 | LR: 2.20e-03\n",
      "Epoch 191 | train MSE: 0.004731 | val MSE: 0.000739 | LR: 2.20e-03\n",
      "Epoch 192 | train MSE: 0.004837 | val MSE: 0.000871 | LR: 2.20e-03\n",
      "Epoch 193 | train MSE: 0.004688 | val MSE: 0.000948 | LR: 2.20e-03\n",
      "Epoch 194 | train MSE: 0.004855 | val MSE: 0.000512 | LR: 2.20e-03\n",
      "Epoch 195 | train MSE: 0.004550 | val MSE: 0.000503 | LR: 2.20e-03\n",
      "Epoch 196 | train MSE: 0.003995 | val MSE: 0.000706 | LR: 2.20e-03\n",
      "Epoch 197 | train MSE: 0.003887 | val MSE: 0.000861 | LR: 2.20e-03\n",
      "Epoch 198 | train MSE: 0.003999 | val MSE: 0.000743 | LR: 2.20e-03\n",
      "Epoch 199 | train MSE: 0.004258 | val MSE: 0.000546 | LR: 2.20e-03\n",
      "Epoch 200 | train MSE: 0.004324 | val MSE: 0.000545 | LR: 2.20e-03\n",
      "Epoch 201 | train MSE: 0.004632 | val MSE: 0.000626 | LR: 2.20e-03\n",
      "Epoch 202 | train MSE: 0.004472 | val MSE: 0.000956 | LR: 2.20e-03\n",
      "Epoch 203 | train MSE: 0.004975 | val MSE: 0.001420 | LR: 2.20e-03\n",
      "Epoch 204 | train MSE: 0.004059 | val MSE: 0.000535 | LR: 2.20e-03\n",
      "Epoch 205 | train MSE: 0.004838 | val MSE: 0.000505 | LR: 2.20e-03\n",
      "Epoch 206 | train MSE: 0.004273 | val MSE: 0.000631 | LR: 2.20e-03\n",
      "Epoch 207 | train MSE: 0.004257 | val MSE: 0.001282 | LR: 2.20e-03\n",
      "Epoch 208 | train MSE: 0.004320 | val MSE: 0.000609 | LR: 2.20e-03\n",
      "Epoch 209 | train MSE: 0.004374 | val MSE: 0.000607 | LR: 2.20e-03\n",
      "Epoch 210 | train MSE: 0.004435 | val MSE: 0.000859 | LR: 2.20e-03\n",
      "Epoch 211 | train MSE: 0.003546 | val MSE: 0.001444 | LR: 2.20e-03\n",
      "Epoch 212 | train MSE: 0.004490 | val MSE: 0.000756 | LR: 2.20e-03\n",
      "Epoch 213 | train MSE: 0.004497 | val MSE: 0.000527 | LR: 2.20e-03\n",
      "Epoch 214 | train MSE: 0.003735 | val MSE: 0.000618 | LR: 2.20e-03\n",
      "Epoch 215 | train MSE: 0.004419 | val MSE: 0.000748 | LR: 2.20e-03\n",
      "Epoch 216 | train MSE: 0.003310 | val MSE: 0.000742 | LR: 2.20e-03\n",
      "Epoch 217 | train MSE: 0.004173 | val MSE: 0.000590 | LR: 2.20e-03\n",
      "Epoch 218 | train MSE: 0.003883 | val MSE: 0.000740 | LR: 2.20e-03\n",
      "Epoch 219 | train MSE: 0.003740 | val MSE: 0.000730 | LR: 2.20e-03\n",
      "Epoch 220 | train MSE: 0.003945 | val MSE: 0.000593 | LR: 2.20e-03\n",
      "Epoch 221 | train MSE: 0.004871 | val MSE: 0.000594 | LR: 2.20e-03\n",
      "Epoch 222 | train MSE: 0.004118 | val MSE: 0.000767 | LR: 2.20e-03\n",
      "Epoch 223 | train MSE: 0.004172 | val MSE: 0.001002 | LR: 2.20e-03\n",
      "Epoch 224 | train MSE: 0.004315 | val MSE: 0.000554 | LR: 2.20e-03\n",
      "Epoch 225 | train MSE: 0.004024 | val MSE: 0.000641 | LR: 2.20e-03\n",
      "Early stopping at epoch 225. Best val MSE: 0.000503\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 250\n",
    "PATIENCE = 30\n",
    "steps_per_epoch = max(1, len(train_loader))\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=EPOCHS, steps_per_epoch=steps_per_epoch)\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "epochs_no_improve = 0\n",
    "losses_history = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # validate\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    losses_history.append([train_loss, val_loss, lr])\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val - 1e-8:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | train MSE: {train_loss:.6f} | val MSE: {val_loss:.6f} | LR: {lr:.2e}\")\n",
    "\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch}. Best val MSE: {best_val:.6f}\")\n",
    "        break\n",
    "\n",
    "\n",
    "# Load best model\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "509a1aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val MSE: 0.000503\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best val MSE: {best_val:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96d6bb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "losses_history = np.array(losses_history)\n",
    "plt.plot(losses_history[:,0], label='Train Loss')\n",
    "plt.plot(losses_history[:,1], label='Val Loss')\n",
    "plt.vlines(np.argmin(losses_history[:,1]), linestyles='--', ymin=0, ymax=max(losses_history[:,1]), label='min_L_val', color='gray')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ea1296",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"losses_history.npy\", losses_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d33836",
   "metadata": {},
   "source": [
    "Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1b1e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.000387\n"
     ]
    }
   ],
   "source": [
    "def evaluate_mse(loader):\n",
    "    model.eval()\n",
    "    mse = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            mse += loss.item() * xb.size(0)\n",
    "    return mse / len(loader.dataset)\n",
    "\n",
    "test_mse = evaluate_mse(test_loader)\n",
    "print(f\"Test MSE: {test_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e86c57",
   "metadata": {},
   "source": [
    "See metrics in original units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bfb3539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE (original units) per target:     [0.00342258]\n",
      "Test MAPE (original units) per target:    [0.34568974] %\n",
      "Test SMAPE (original units) per target:   [0.3477067] %\n",
      "Test RMSE (original units) per target:    [0.00683967]\n",
      "Test MSE (original units) per target:     [4.6781097e-05]\n"
     ]
    }
   ],
   "source": [
    "DTYPE = np.float32\n",
    "\n",
    "def fit_minmax(X):\n",
    "    \"\"\"Fit per-feature min-max on X (2D). Returns (mins, ranges) with safe ranges.\"\"\"\n",
    "    mins = X.min(axis=0)\n",
    "    maxs = X.max(axis=0)\n",
    "    rng = maxs - mins\n",
    "    rng_safe = np.where(rng > 0, rng, 1.0)  # avoid division by zero (constant features)\n",
    "    return mins.astype(DTYPE), rng_safe.astype(DTYPE)\n",
    "\n",
    "def transform_minmax(X, mins, rng_safe):\n",
    "    return ((X - mins) / rng_safe).astype(DTYPE)\n",
    "\n",
    "def inverse_minmax(X_scaled, mins, rng_safe):\n",
    "    return (X_scaled * rng_safe + mins).astype(DTYPE)\n",
    "\n",
    "try:\n",
    "    mm = np.load(\"saved_models/dnn_7_(lam)/minmax_params_dnn_7_(lam).npz\")\n",
    "    y_mins, y_rng = mm[\"y_mins\"], mm[\"y_rng\"]\n",
    "\n",
    "    # Compute MAE/RMSE in original units\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            pred = model(xb).cpu().numpy()\n",
    "            preds.append(pred)\n",
    "            trues.append(yb.numpy())\n",
    "    preds = np.vstack(preds)\n",
    "    trues = np.vstack(trues)\n",
    "\n",
    "    preds_u = inverse_minmax(preds, y_mins, y_rng)\n",
    "    trues_u = inverse_minmax(trues, y_mins, y_rng)\n",
    "\n",
    "    mae = np.mean(np.abs(preds_u - trues_u), axis=0)\n",
    "    mape = 100 * np.mean(np.abs((preds_u - trues_u) / trues_u), axis=0)\n",
    "    epsilon = 1e-8\n",
    "    smape = 100 * np.mean(2 * np.abs(preds_u - trues_u) / (np.abs(preds_u) + np.abs(trues_u) + epsilon), axis=0)\n",
    "    rmse = np.sqrt(np.mean((preds_u - trues_u) ** 2, axis=0))\n",
    "    mse = np.mean((preds_u - trues_u) ** 2, axis=0)\n",
    "    print(\"Test MAE (original units) per target:    \", mae)\n",
    "    print(\"Test MAPE (original units) per target:   \", mape, '%')\n",
    "    print(\"Test SMAPE (original units) per target:  \", smape, '%')\n",
    "    print(\"Test RMSE (original units) per target:   \", rmse)\n",
    "    print(\"Test MSE (original units) per target:    \", mse)\n",
    "except FileNotFoundError:\n",
    "    print(\"minmax_params.npz not found; skipping metrics in original units.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc98951",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c41bf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to dnn_7_lam.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"dnn_7_lam.pt\")\n",
    "print(\"Saved model to dnn_7_lam.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5777d529",
   "metadata": {},
   "source": [
    "### 3.\n",
    "See results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c964f95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction lambda:  [0.92958724]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "# Load saved model if needed\n",
    "\n",
    "# hidden  = [192, 384, 384, 192]\n",
    "# dropout = 0.05701\n",
    "# activation = 'gelu'\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# in_dim = 16\n",
    "# out_dim = 2\n",
    "# model = DNN(in_dim, hidden, out_dim, activation=activation, dropout=dropout).to(device)\n",
    "# model.load_state_dict(torch.load(\"saved_models/dnn_6_(dpz)/dnn_6_dpz.pt\"))\n",
    "model.eval()  # set to evaluation mode\n",
    "\n",
    "\n",
    "dV_ges    = 280       # original units (as in CSV)\n",
    "eps_0     = 0.2\n",
    "phi_0     = 180e-6\n",
    "h_dis_0   = 0.04\n",
    "h_c_0     = 0.1\n",
    "rho_c     = 998.19\n",
    "rho_d     = 819.72\n",
    "eta_c     = 102e-5\n",
    "eta_d     = 973e-5\n",
    "sigma     = 0.036\n",
    "T         = 20\n",
    "r_s_star  = 0.00975\n",
    "h_p_star  = 0.2498\n",
    "D_A       = 0.15\n",
    "L_A       = 0.56\n",
    "\n",
    "# Apply the SAME transformations you used when building X in training:\n",
    "# dV_ges -> /3.6 * 1e-6 ; DPZ_pos was in meters AFTER dividing by 100 in the CSV pipeline.\n",
    "# Here, x_array is already in meters, so no extra /100.\n",
    "dV_ges_tr = dV_ges / 3.6 * 1e-6\n",
    "\n",
    "X_real = np.array(\n",
    "    [dV_ges_tr, eps_0, phi_0, h_dis_0, h_c_0,\n",
    "     rho_c, rho_d, eta_c, eta_d, sigma, T, r_s_star, h_p_star, D_A, L_A],\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "\n",
    "X_scaled = transform_minmax(X_real, x_mins, x_rng)\n",
    "X_t = torch.from_numpy(X_scaled).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    Y_scaled = model(X_t).cpu().numpy()\n",
    "\n",
    "def transform_minmax(X, mins, rng):\n",
    "    rng_safe = np.where(rng > 0, rng, 1.0)\n",
    "    return (X - mins) / rng_safe\n",
    "\n",
    "def inverse_minmax(Ys, mins, rng):\n",
    "    return Ys * rng + mins\n",
    "\n",
    "Y_pred = inverse_minmax(Y_scaled, y_mins, y_rng)  \n",
    "\n",
    "print('Prediction lambda: ', Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527636a",
   "metadata": {},
   "source": [
    "### 5.\n",
    "Optuna analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd10b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iagr9\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\iagr9\\AppData\\Local\\Temp\\ipykernel_19972\\2171709183.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_data = torch.load(DATA_TORCH_PATH)\n",
      "c:\\Users\\iagr9\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "c:\\Users\\iagr9\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\_experimental.py:32: ExperimentalWarning: Argument ``group`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "[I 2025-08-26 15:06:01,526] A new study created in memory with name: dnn_optuna_t_20250826_150601\n",
      "Best trial: 0. Best value: 0.000358216:   2%|â–         | 1/60 [00:05<05:17,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:06,771] Trial 0 finished with value: 0.0003582162435277034 and parameters: {'n_layers': 2, 'width_l1': 64, 'width_l2': 192, 'activation': 'silu', 'dropout': 0.14561457009902096, 'lr': 0.0003278187653397617, 'weight_decay': 6.870101665590006e-08, 'batch_size': 512, 'optimizer': 'adamw', 'max_epochs': 181}. Best is trial 0 with value: 0.0003582162435277034.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.000358216:   3%|â–Ž         | 2/60 [00:09<04:15,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:10,475] Trial 1 finished with value: 0.029994726181030273 and parameters: {'n_layers': 1, 'width_l1': 256, 'activation': 'gelu', 'dropout': 0.2475884550556351, 'lr': 1.216702881459345e-05, 'weight_decay': 0.0028570800750407216, 'batch_size': 128, 'optimizer': 'adam', 'max_epochs': 245}. Best is trial 0 with value: 0.0003582162435277034.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.000358216:   5%|â–Œ         | 3/60 [00:11<03:06,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:12,406] Trial 2 finished with value: 0.0004866657368207382 and parameters: {'n_layers': 4, 'width_l1': 64, 'width_l2': 192, 'width_l3': 192, 'width_l4': 64, 'activation': 'relu', 'dropout': 0.1554911608578311, 'lr': 6.390259853593123e-05, 'weight_decay': 0.00023858166771428845, 'batch_size': 128, 'optimizer': 'adamw', 'max_epochs': 175}. Best is trial 0 with value: 0.0003582162435277034.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.000358216:   7%|â–‹         | 4/60 [00:13<02:51,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:15,154] Trial 3 finished with value: 0.002433788220761186 and parameters: {'n_layers': 4, 'width_l1': 512, 'width_l2': 128, 'width_l3': 192, 'width_l4': 384, 'activation': 'tanh', 'dropout': 0.4303652916281717, 'lr': 1.0404501336028154e-05, 'weight_decay': 1.160068983900714e-05, 'batch_size': 64, 'optimizer': 'adam', 'max_epochs': 168}. Best is trial 0 with value: 0.0003582162435277034.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.000358216:   8%|â–Š         | 5/60 [00:16<02:36,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:17,595] Trial 4 finished with value: 0.04014920380155919 and parameters: {'n_layers': 3, 'width_l1': 96, 'width_l2': 384, 'width_l3': 128, 'activation': 'relu', 'dropout': 0.26788734203737924, 'lr': 1.673627134612419e-05, 'weight_decay': 0.0010275784161907387, 'batch_size': 512, 'optimizer': 'adam', 'max_epochs': 167}. Best is trial 0 with value: 0.0003582162435277034.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 5. Best value: 0.000186522:  10%|â–ˆ         | 6/60 [00:17<02:10,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:19,217] Trial 5 finished with value: 0.0001865216388793315 and parameters: {'n_layers': 1, 'width_l1': 256, 'activation': 'gelu', 'dropout': 0.3299920230170895, 'lr': 0.0010576902036995882, 'weight_decay': 2.14390170468164e-05, 'batch_size': 512, 'optimizer': 'adam', 'max_epochs': 137}. Best is trial 5 with value: 0.0001865216388793315.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 5. Best value: 0.000186522:  12%|â–ˆâ–        | 7/60 [00:18<01:32,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:19,577] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 5. Best value: 0.000186522:  13%|â–ˆâ–Ž        | 8/60 [00:18<01:09,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:20,019] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.000164605:  15%|â–ˆâ–Œ        | 9/60 [00:19<01:06,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:21,296] Trial 8 finished with value: 0.0001646047773755203 and parameters: {'n_layers': 2, 'width_l1': 128, 'width_l2': 64, 'activation': 'tanh', 'dropout': 0.43353615929005185, 'lr': 0.0018289742175371328, 'weight_decay': 1.1696458740181163e-05, 'batch_size': 128, 'optimizer': 'adamw', 'max_epochs': 137}. Best is trial 8 with value: 0.0001646047773755203.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 9. Best value: 0.000118862:  17%|â–ˆâ–‹        | 10/60 [00:23<01:35,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:24,519] Trial 9 finished with value: 0.0001188615037096759 and parameters: {'n_layers': 2, 'width_l1': 512, 'width_l2': 128, 'activation': 'gelu', 'dropout': 0.265677315784074, 'lr': 0.00021838296130342758, 'weight_decay': 6.676969754945983e-05, 'batch_size': 128, 'optimizer': 'adam', 'max_epochs': 155}. Best is trial 9 with value: 0.0001188615037096759.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 9. Best value: 0.000118862:  18%|â–ˆâ–Š        | 11/60 [00:24<01:25,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:25,872] Trial 10 finished with value: 0.00013785168402275796 and parameters: {'n_layers': 1, 'width_l1': 96, 'activation': 'silu', 'dropout': 0.13996694847297142, 'lr': 0.002319087217474401, 'weight_decay': 0.0002675355506980832, 'batch_size': 128, 'optimizer': 'adamw', 'max_epochs': 82}. Best is trial 9 with value: 0.0001188615037096759.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 9. Best value: 0.000118862:  20%|â–ˆâ–ˆ        | 12/60 [00:24<01:03,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:26,204] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 9. Best value: 0.000118862:  20%|â–ˆâ–ˆ        | 12/60 [00:27<01:03,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:29,457] Trial 12 finished with value: 0.00011139194149587114 and parameters: {'n_layers': 3, 'width_l1': 128, 'width_l2': 192, 'width_l3': 512, 'activation': 'gelu', 'dropout': 0.3483685826820753, 'lr': 0.0003613829791328161, 'weight_decay': 0.0018400604093141224, 'batch_size': 128, 'optimizer': 'adamw', 'max_epochs': 249}. Best is trial 12 with value: 0.00011139194149587114.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.000105061:  23%|â–ˆâ–ˆâ–Ž       | 14/60 [00:30<01:28,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:31,437] Trial 13 finished with value: 0.00010506096059235476 and parameters: {'n_layers': 2, 'width_l1': 192, 'width_l2': 128, 'activation': 'silu', 'dropout': 0.15965681879520743, 'lr': 0.0022564135127852, 'weight_decay': 0.005054088956519247, 'batch_size': 128, 'optimizer': 'adamw', 'max_epochs': 208}. Best is trial 13 with value: 0.00010506096059235476.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.000105061:  25%|â–ˆâ–ˆâ–Œ       | 15/60 [00:30<01:11,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:32,229] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.000105061:  27%|â–ˆâ–ˆâ–‹       | 16/60 [00:34<01:34,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:35,720] Trial 15 finished with value: 0.00011471189337514214 and parameters: {'n_layers': 3, 'width_l1': 96, 'width_l2': 512, 'width_l3': 512, 'activation': 'gelu', 'dropout': 0.2786929334308535, 'lr': 0.00070314002705525, 'weight_decay': 0.0004680161743980331, 'batch_size': 128, 'optimizer': 'adamw', 'max_epochs': 244}. Best is trial 13 with value: 0.00010506096059235476.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.000105061:  28%|â–ˆâ–ˆâ–Š       | 17/60 [00:35<01:26,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:37,367] Trial 16 finished with value: 0.00010959897979588832 and parameters: {'n_layers': 2, 'width_l1': 128, 'width_l2': 256, 'activation': 'silu', 'dropout': 0.1519769330734731, 'lr': 0.0026719206310586254, 'weight_decay': 0.00031454577291088, 'batch_size': 64, 'optimizer': 'adamw', 'max_epochs': 210}. Best is trial 13 with value: 0.00010506096059235476.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 17. Best value: 0.000100812:  30%|â–ˆâ–ˆâ–ˆ       | 18/60 [00:38<01:35,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:40,249] Trial 17 finished with value: 0.00010081246432106373 and parameters: {'n_layers': 2, 'width_l1': 512, 'width_l2': 256, 'activation': 'silu', 'dropout': 0.14057161246539154, 'lr': 0.0027193920532871214, 'weight_decay': 0.0017385150199212337, 'batch_size': 64, 'optimizer': 'adamw', 'max_epochs': 206}. Best is trial 17 with value: 0.00010081246432106373.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 17. Best value: 0.000100812:  30%|â–ˆâ–ˆâ–ˆ       | 18/60 [00:40<01:35,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:41,839] Trial 18 finished with value: 0.00011167676835241965 and parameters: {'n_layers': 2, 'width_l1': 192, 'width_l2': 96, 'activation': 'tanh', 'dropout': 0.17465836597183534, 'lr': 0.0011285810730822924, 'weight_decay': 0.005212850896976305, 'batch_size': 128, 'optimizer': 'adamw', 'max_epochs': 226}. Best is trial 17 with value: 0.00010081246432106373.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 17. Best value: 0.000100812:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 20/60 [00:40<01:04,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:42,394] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: 0.000100314:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/60 [00:42<01:01,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:43,909] Trial 20 finished with value: 0.00010031406467748901 and parameters: {'n_layers': 1, 'width_l1': 512, 'activation': 'relu', 'dropout': 0.030416598855053156, 'lr': 0.0018890096154582253, 'weight_decay': 0.0016335881943154646, 'batch_size': 64, 'optimizer': 'adam', 'max_epochs': 177}. Best is trial 20 with value: 0.00010031406467748901.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: 0.000100314:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/60 [00:43<00:55,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:45,099] Trial 21 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: 0.000100314:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 23/60 [00:44<00:42,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:45,489] Trial 22 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 23. Best value: 7.55225e-05:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/60 [00:45<00:47,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:47,223] Trial 23 finished with value: 7.552254976610006e-05 and parameters: {'n_layers': 1, 'width_l1': 512, 'activation': 'relu', 'dropout': 0.05576845714824472, 'lr': 0.0017461908568397002, 'weight_decay': 1.7273944145744986e-05, 'batch_size': 64, 'optimizer': 'adamw', 'max_epochs': 180}. Best is trial 23 with value: 7.552254976610006e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 23. Best value: 7.55225e-05:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/60 [00:46<00:35,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:47,573] Trial 24 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 23. Best value: 7.55225e-05:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 26/60 [00:46<00:29,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:48,069] Trial 25 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 23. Best value: 7.55225e-05:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/60 [00:50<00:58,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:51,943] Trial 26 finished with value: 8.85439941943702e-05 and parameters: {'n_layers': 1, 'width_l1': 512, 'activation': 'gelu', 'dropout': 0.012032161043447509, 'lr': 0.0023255970635084056, 'weight_decay': 6.576355661962362e-05, 'batch_size': 64, 'optimizer': 'adam', 'max_epochs': 169}. Best is trial 23 with value: 7.552254976610006e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 7.27745e-05:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/60 [00:54<01:12,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:55,391] Trial 27 finished with value: 7.277449308815649e-05 and parameters: {'n_layers': 2, 'width_l1': 256, 'width_l2': 96, 'activation': 'gelu', 'dropout': 0.005741066177104841, 'lr': 0.0022686781521679966, 'weight_decay': 8.903579792084849e-06, 'batch_size': 64, 'optimizer': 'adam', 'max_epochs': 135}. Best is trial 27 with value: 7.277449308815649e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 7.27745e-05:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 29/60 [00:56<01:08,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:06:57,496] Trial 28 finished with value: 0.00010187257775815867 and parameters: {'n_layers': 2, 'width_l1': 256, 'width_l2': 96, 'activation': 'gelu', 'dropout': 0.037478836287380704, 'lr': 0.002007542991650768, 'weight_decay': 1.8933815265336567e-05, 'batch_size': 64, 'optimizer': 'adam', 'max_epochs': 103}. Best is trial 27 with value: 7.277449308815649e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/60 [01:01<01:31,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:02,441] Trial 29 finished with value: 6.217337844861766e-05 and parameters: {'n_layers': 1, 'width_l1': 512, 'activation': 'gelu', 'dropout': 0.10815580728807075, 'lr': 0.0024259328023214497, 'weight_decay': 6.771846902062542e-06, 'batch_size': 64, 'optimizer': 'adam', 'max_epochs': 231}. Best is trial 29 with value: 6.217337844861766e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/60 [01:03<01:26,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:05,321] Trial 30 finished with value: 0.00010391718597482827 and parameters: {'n_layers': 2, 'width_l1': 512, 'width_l2': 96, 'activation': 'gelu', 'dropout': 0.25332044272743137, 'lr': 0.00042281855952601237, 'weight_decay': 5.2250982993552795e-06, 'batch_size': 64, 'optimizer': 'adam', 'max_epochs': 248}. Best is trial 29 with value: 6.217337844861766e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/60 [01:04<01:26,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:05,722] Trial 31 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/60 [01:04<00:46,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:06,263] Trial 32 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 34/60 [01:05<00:34,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:06,747] Trial 33 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 35/60 [01:05<00:26,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:07,185] Trial 34 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/60 [01:06<00:22,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:07,793] Trial 35 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/60 [01:09<00:22,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:11,273] Trial 36 finished with value: 7.319258589108112e-05 and parameters: {'n_layers': 1, 'width_l1': 512, 'activation': 'gelu', 'dropout': 0.03995332982632552, 'lr': 0.00198494126090456, 'weight_decay': 0.00020770884579983734, 'batch_size': 64, 'optimizer': 'adamw', 'max_epochs': 162}. Best is trial 29 with value: 6.217337844861766e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/60 [01:10<00:39,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:11,735] Trial 37 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/60 [01:10<00:23,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:12,316] Trial 38 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 40/60 [01:12<00:23,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:13,737] Trial 39 finished with value: 0.00010750941553358304 and parameters: {'n_layers': 1, 'width_l1': 512, 'activation': 'gelu', 'dropout': 0.012706320769401373, 'lr': 0.0024113456521507423, 'weight_decay': 0.00012734577708550446, 'batch_size': 128, 'optimizer': 'adamw', 'max_epochs': 167}. Best is trial 29 with value: 6.217337844861766e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 41/60 [01:12<00:18,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:14,164] Trial 40 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/60 [01:13<00:18,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:15,378] Trial 41 finished with value: 0.00010673569168074656 and parameters: {'n_layers': 2, 'width_l1': 256, 'width_l2': 96, 'activation': 'tanh', 'dropout': 0.010980172403148573, 'lr': 0.0026637534702614075, 'weight_decay': 0.00021766948019197075, 'batch_size': 64, 'optimizer': 'adamw', 'max_epochs': 130}. Best is trial 29 with value: 6.217337844861766e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 43/60 [01:16<00:27,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:18,299] Trial 42 finished with value: 9.346921470458225e-05 and parameters: {'n_layers': 1, 'width_l1': 512, 'activation': 'gelu', 'dropout': 0.06681186172389753, 'lr': 0.0016989951578234158, 'weight_decay': 5.39257684183699e-05, 'batch_size': 64, 'optimizer': 'adam', 'max_epochs': 188}. Best is trial 29 with value: 6.217337844861766e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 44/60 [01:17<00:20,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:18,860] Trial 43 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 45/60 [01:17<00:15,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:19,293] Trial 44 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 46/60 [01:18<00:11,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:19,709] Trial 45 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 47/60 [01:18<00:10,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:20,328] Trial 46 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 48/60 [01:19<00:08,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:20,791] Trial 47 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 49/60 [01:19<00:06,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:21,096] Trial 48 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 29. Best value: 6.21734e-05:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 49/60 [01:22<00:06,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:24,345] Trial 49 finished with value: 6.0687974190055316e-05 and parameters: {'n_layers': 2, 'width_l1': 512, 'width_l2': 64, 'activation': 'gelu', 'dropout': 0.12205115187441971, 'lr': 0.002919561339826874, 'weight_decay': 1.2836697830950576e-08, 'batch_size': 64, 'optimizer': 'adam', 'max_epochs': 231}. Best is trial 49 with value: 6.0687974190055316e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 6.0688e-05:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 51/60 [01:24<00:13,  1.47s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:26,059] Trial 50 finished with value: 0.00010178951641260567 and parameters: {'n_layers': 2, 'width_l1': 512, 'width_l2': 64, 'activation': 'silu', 'dropout': 0.196110049049052, 'lr': 0.0027736405939628507, 'weight_decay': 1.0900454359364092e-08, 'batch_size': 64, 'optimizer': 'adam', 'max_epochs': 227}. Best is trial 49 with value: 6.0687974190055316e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 6.0688e-05:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 52/60 [01:25<00:09,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:26,389] Trial 51 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 6.0688e-05:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 53/60 [01:28<00:13,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:29,959] Trial 52 finished with value: 6.715163304391554e-05 and parameters: {'n_layers': 2, 'width_l1': 512, 'width_l2': 64, 'activation': 'gelu', 'dropout': 0.047026319780203385, 'lr': 0.0024962358645294596, 'weight_decay': 5.487884428874365e-07, 'batch_size': 64, 'optimizer': 'adam', 'max_epochs': 221}. Best is trial 49 with value: 6.0687974190055316e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 6.0688e-05:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 54/60 [01:28<00:08,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:30,382] Trial 53 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 6.0688e-05:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 55/60 [01:31<00:08,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:32,872] Trial 54 finished with value: 9.031102882098344e-05 and parameters: {'n_layers': 2, 'width_l1': 384, 'width_l2': 64, 'activation': 'relu', 'dropout': 0.012043036056179707, 'lr': 0.000902082728272321, 'weight_decay': 4.3428018916406133e-07, 'batch_size': 64, 'optimizer': 'adam', 'max_epochs': 215}. Best is trial 49 with value: 6.0687974190055316e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 6.0688e-05:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 56/60 [01:32<00:05,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:33,498] Trial 55 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 49. Best value: 6.0688e-05:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 57/60 [01:32<00:03,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:33,890] Trial 56 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 57. Best value: 5.92306e-05:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 58/60 [01:36<00:04,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:38,348] Trial 57 finished with value: 5.923062799719431e-05 and parameters: {'n_layers': 3, 'width_l1': 192, 'width_l2': 64, 'width_l3': 96, 'activation': 'gelu', 'dropout': 0.0357751751540505, 'lr': 0.002295476569774029, 'weight_decay': 1.6234113691658224e-06, 'batch_size': 64, 'optimizer': 'adam', 'max_epochs': 226}. Best is trial 57 with value: 5.923062799719431e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 57. Best value: 5.92306e-05:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 59/60 [01:42<00:03,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:43,566] Trial 58 finished with value: 0.00010105344798352759 and parameters: {'n_layers': 4, 'width_l1': 192, 'width_l2': 64, 'width_l3': 384, 'width_l4': 192, 'activation': 'gelu', 'dropout': 0.042738799577730746, 'lr': 0.0006262026140547451, 'weight_decay': 2.2405592767428494e-06, 'batch_size': 64, 'optimizer': 'adam', 'max_epochs': 218}. Best is trial 57 with value: 5.923062799719431e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 57. Best value: 5.92306e-05: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [01:42<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 15:07:43,963] Trial 59 pruned. \n",
      "\n",
      "Study statistics:\n",
      "  Completed trials: 60\n",
      "  Pruned trials: 28\n",
      "  Successful trials: 32\n",
      "\n",
      "Best trial:\n",
      "  Value (Val MSE, original scale): 0.000059\n",
      "  Params:\n",
      "    n_layers: 3\n",
      "    width_l1: 192\n",
      "    width_l2: 64\n",
      "    width_l3: 96\n",
      "    activation: gelu\n",
      "    dropout: 0.0357751751540505\n",
      "    lr: 0.002295476569774029\n",
      "    weight_decay: 1.6234113691658224e-06\n",
      "    batch_size: 64\n",
      "    optimizer: adam\n",
      "    max_epochs: 226\n",
      "\n",
      "Test MSE (original scale): 0.000042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iagr9\\AppData\\Local\\Temp\\ipykernel_19972\\2171709183.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(state_path, map_location=DEVICE))\n"
     ]
    }
   ],
   "source": [
    "# tune_dnn_optuna.py\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "# --------------------\n",
    "# Config\n",
    "# --------------------\n",
    "DATA_BUNDLE_DIR = \"saved_models/dnn_7_(lam)\"\n",
    "DATA_TORCH_PATH = os.path.join(DATA_BUNDLE_DIR, \"datasets_dnn_7_(lam).pt\")\n",
    "MM_PARAMS_NPZ   = os.path.join(DATA_BUNDLE_DIR, \"minmax_params_dnn_7_(lam).npz\")\n",
    "RESULTS_DIR     = os.path.join(DATA_BUNDLE_DIR, \"optuna_runs\")\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True  # faster on GPU\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Choose which split variant to use: \"t\" or \"n\"\n",
    "SPLIT_VARIANT = \"t\"  # change to \"n\" if you want *_n\n",
    "\n",
    "# --------------------\n",
    "# Load data\n",
    "# --------------------\n",
    "if not os.path.exists(DATA_TORCH_PATH):\n",
    "    raise FileNotFoundError(f\"Could not find {DATA_TORCH_PATH}\")\n",
    "\n",
    "loaded_data = torch.load(DATA_TORCH_PATH)\n",
    "\n",
    "X_train = loaded_data[f\"X_train_{SPLIT_VARIANT}\"].float()\n",
    "Y_train = loaded_data[f\"Y_train_{SPLIT_VARIANT}\"].float()\n",
    "X_val   = loaded_data[f\"X_val_{SPLIT_VARIANT}\"].float()\n",
    "Y_val   = loaded_data[f\"Y_val_{SPLIT_VARIANT}\"].float()\n",
    "X_test  = loaded_data[f\"X_test_{SPLIT_VARIANT}\"].float()\n",
    "Y_test  = loaded_data[f\"Y_test_{SPLIT_VARIANT}\"].float()\n",
    "\n",
    "mm = np.load(MM_PARAMS_NPZ)\n",
    "y_mins = torch.tensor(mm[\"y_mins\"], dtype=torch.float32)\n",
    "y_rng  = torch.tensor(mm[\"y_rng\"],  dtype=torch.float32)\n",
    "\n",
    "in_dim  = X_train.shape[1]\n",
    "out_dim = Y_train.shape[1]\n",
    "\n",
    "# --------------------\n",
    "# Model definition (copied from your DNN with same init)\n",
    "# --------------------\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims, out_dim, activation=\"silu\", dropout=0.0):\n",
    "        super().__init__()\n",
    "        acts = {\n",
    "            \"relu\": nn.ReLU,\n",
    "            \"gelu\": nn.GELU,\n",
    "            \"silu\": nn.SiLU,\n",
    "            \"tanh\": nn.Tanh\n",
    "        }\n",
    "        if activation not in acts:\n",
    "            raise ValueError(f\"activation must be one of {list(acts.keys())}\")\n",
    "        Act = acts[activation]\n",
    "\n",
    "        layers = []\n",
    "        prev = in_dim\n",
    "        for h in hidden_dims:\n",
    "            layers += [nn.Linear(prev, h), Act()]\n",
    "            if dropout and dropout > 0:\n",
    "                layers += [nn.Dropout(p=dropout)]\n",
    "            prev = h\n",
    "        layers += [nn.Linear(prev, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if activation in (\"relu\", \"silu\", \"gelu\"):\n",
    "                    nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "                else:\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n",
    "                    bound = 1 / math.sqrt(fan_in)\n",
    "                    nn.init.uniform_(m.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --------------------\n",
    "# Helpers\n",
    "# --------------------\n",
    "def denorm_y(y_norm: torch.Tensor) -> torch.Tensor:\n",
    "    # y = y_norm * y_rng + y_mins (broadcast over batch)\n",
    "    return y_norm * y_rng.to(y_norm.device) + y_mins.to(y_norm.device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def mse_original_scale(model: nn.Module, loader: DataLoader, device=DEVICE) -> float:\n",
    "    model.eval()\n",
    "    se_sum, n_obs = 0.0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        pred = model(xb)\n",
    "        pred_den = denorm_y(pred)\n",
    "        y_den    = denorm_y(yb)\n",
    "        se_sum += torch.sum((pred_den - y_den) ** 2).item()\n",
    "        n_obs  += yb.numel()\n",
    "    mse = se_sum / n_obs\n",
    "    return float(mse)\n",
    "\n",
    "def make_hidden_dims(trial: optuna.Trial) -> list:\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
    "    dims = []\n",
    "    for i in range(n_layers):\n",
    "        dims.append(trial.suggest_categorical(f\"width_l{i+1}\", [64, 96, 128, 192, 256, 384, 512]))\n",
    "    return dims\n",
    "\n",
    "def make_optimizer(opt_name: str, params, lr: float, wd: float):\n",
    "    if opt_name == \"adam\":\n",
    "        return torch.optim.Adam(params, lr=lr, weight_decay=wd)\n",
    "    elif opt_name == \"adamw\":\n",
    "        return torch.optim.AdamW(params, lr=lr, weight_decay=wd)\n",
    "    else:\n",
    "        raise ValueError(opt_name)\n",
    "\n",
    "# --------------------\n",
    "# Objective\n",
    "# --------------------\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    # Search space\n",
    "    hidden_dims = make_hidden_dims(trial)\n",
    "    activation  = trial.suggest_categorical(\"activation\", [\"relu\", \"gelu\", \"silu\", \"tanh\"])\n",
    "    dropout     = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "    lr          = trial.suggest_float(\"lr\", 1e-5, 3e-3, log=True)\n",
    "    weight_decay= trial.suggest_float(\"weight_decay\", 1e-8, 1e-2, log=True)\n",
    "    batch_size  = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512])\n",
    "    optimizer_n = trial.suggest_categorical(\"optimizer\", [\"adam\", \"adamw\"])\n",
    "    max_epochs  = trial.suggest_int(\"max_epochs\", 80, 250)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_loader   = DataLoader(TensorDataset(X_val,   Y_val),   batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    # Model / opt / loss\n",
    "    model = DNN(in_dim, hidden_dims, out_dim, activation=activation, dropout=dropout).to(DEVICE)\n",
    "    opt   = make_optimizer(optimizer_n, model.parameters(), lr, weight_decay)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # OneCycleLR (optional): only if epochs >= 50\n",
    "    use_scheduler = max_epochs >= 50\n",
    "    if use_scheduler:\n",
    "        # steps_per_epoch = max(1, len(train_loader))\n",
    "        # scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=lr, epochs=max_epochs, steps_per_epoch=steps_per_epoch)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.1, patience=30)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # Early stopping + pruning\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    patience = 30\n",
    "    patience_left = patience\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "\n",
    "        # Validation metric (original scale MSE)\n",
    "        val_mse = mse_original_scale(model, val_loader, device=DEVICE)\n",
    "        if scheduler:\n",
    "            scheduler.step(val_mse)\n",
    "\n",
    "        # Report to Optuna and check pruning\n",
    "        trial.report(val_mse, step=epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        # Early stopping tracking\n",
    "        if val_mse < best_val - 1e-6:\n",
    "            best_val = val_mse\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_left = patience\n",
    "        else:\n",
    "            patience_left -= 1\n",
    "            if patience_left <= 0:\n",
    "                break\n",
    "\n",
    "    # Optionally save the trial's best model\n",
    "    trial_dir = os.path.join(RESULTS_DIR, f\"trial_{trial.number:04d}\")\n",
    "    os.makedirs(trial_dir, exist_ok=True)\n",
    "    with open(os.path.join(trial_dir, \"params.json\"), \"w\") as f:\n",
    "        json.dump(trial.params, f, indent=2)\n",
    "    if best_state is not None:\n",
    "        torch.save(best_state, os.path.join(trial_dir, \"best_state.pt\"))\n",
    "\n",
    "    return best_val\n",
    "\n",
    "# --------------------\n",
    "# Run Study\n",
    "# --------------------\n",
    "def main():\n",
    "    study_name = f\"dnn_optuna_{SPLIT_VARIANT}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    storage = None  # e.g., \"sqlite:///optuna_dnn.db\" if you want persistence\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        direction=\"minimize\",\n",
    "        storage=storage,\n",
    "        load_if_exists=False,\n",
    "        pruner=MedianPruner(n_warmup_steps=15),\n",
    "        sampler=optuna.samplers.TPESampler(seed=SEED, n_startup_trials=15, multivariate=True, group=True),\n",
    "    )\n",
    "\n",
    "    # You can adjust n_trials as you like\n",
    "    study.optimize(objective, n_trials=60, show_progress_bar=True, gc_after_trial=True)\n",
    "\n",
    "    print(\"\\nStudy statistics:\")\n",
    "    print(f\"  Completed trials: {len(study.trials)}\")\n",
    "    pruned_trials = [t for t in study.trials if t.state == TrialState.PRUNED]\n",
    "    complete_trials = [t for t in study.trials if t.state == TrialState.COMPLETE]\n",
    "    print(f\"  Pruned trials: {len(pruned_trials)}\")\n",
    "    print(f\"  Successful trials: {len(complete_trials)}\")\n",
    "\n",
    "    print(\"\\nBest trial:\")\n",
    "    best = study.best_trial\n",
    "    print(f\"  Value (Val MSE, original scale): {best.value:.6f}\")\n",
    "    print(\"  Params:\")\n",
    "    for k, v in best.params.items():\n",
    "        print(f\"    {k}: {v}\")\n",
    "\n",
    "    # Save best params + optionally evaluate on test set using saved state\n",
    "    best_dir = os.path.join(RESULTS_DIR, \"best\")\n",
    "    os.makedirs(best_dir, exist_ok=True)\n",
    "    with open(os.path.join(best_dir, \"best_params.json\"), \"w\") as f:\n",
    "        json.dump(best.params, f, indent=2)\n",
    "\n",
    "    # Rebuild best model and evaluate on test\n",
    "    hidden_dims = [best.params[f\"width_l{i+1}\"] for i in range(best.params[\"n_layers\"])]\n",
    "    model = DNN(in_dim, hidden_dims, out_dim,\n",
    "                activation=best.params[\"activation\"],\n",
    "                dropout=best.params[\"dropout\"]).to(DEVICE)\n",
    "\n",
    "    # Load the saved state from the best trial directory if present; otherwise use current weights\n",
    "    best_trial_dir = os.path.join(RESULTS_DIR, f\"trial_{best.number:04d}\")\n",
    "    state_path = os.path.join(best_trial_dir, \"best_state.pt\")\n",
    "    if os.path.exists(state_path):\n",
    "        model.load_state_dict(torch.load(state_path, map_location=DEVICE))\n",
    "\n",
    "    test_loader = DataLoader(TensorDataset(X_test, Y_test), batch_size=4096, shuffle=False)\n",
    "    test_mse = mse_original_scale(model, test_loader, device=DEVICE)\n",
    "    print(f\"\\nTest MSE (original scale): {test_mse:.6f}\")\n",
    "    with open(os.path.join(best_dir, \"best_summary.json\"), \"w\") as f:\n",
    "        json.dump({\"val_mse\": best.value, \"test_mse\": test_mse, \"trial_number\": best.number}, f, indent=2)\n",
    "\n",
    "    # Save final best model weights\n",
    "    torch.save(model.state_dict(), os.path.join(best_dir, \"best_model_state.pt\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
